\begin{center}
\indent
\textit{Rappresentazione con matrici, diagonalizzazione.}
\end{center}

\section{Applicazione lineari}

\begin{defn}[Applicazione lineare]
Le applicazioni lineari sono morfismi tra spazi vettoriali sullo stesso campo $\field$. Un'applicazione $L : V \to V'$ \`e lineare se conserva tutte le propriet\`a degli spazi vettoriali. Deve quindi conservare le operazioni:
\begin{enumerate}
    \item $L(v + w) = L(v) + L(w)$
    \item $ \forall a \in \field$ e $\forall v \in V$, $L(a \cdot v) = a \cdot L(v)$
\end{enumerate}
Equivalentemente possiamo dire che:
\[
\forall a, b \in \field , \, \forall v, w \in V , \, L(a \cdot v + b \cdot w) = a \cdot L(v) + b \cdot L(w)
\]
Conserva la linearit\`a: manda una combinazione lineare nella combinazione lineare delle immagini con gli stessi coefficienti.
\end{defn}
\begin{proof}
Vediamolo da sinistra verso destra, $L(a \cdot v + b \cdot w) = L(a \cdot v) + L(b \cdot w) = a \cdot L(v) + b \cdot L(w)$, per le due propriet\`a delle applicazioni lineari.

Viceversa, $L(a \cdot v + 0 \cdot w) = L(a \cdot v) = a \cdot L(v)$, e $L(1 \cdot v + 1 \cdot w) = 1 \cdot L(v) + 1 \cdot L(w) = L(v) + L(w)$.
\end{proof}

Un morfismo di strutture algebriche individua un nucleo e un'immagine. Un'applicazione lineare $L: V \to V'$ quindi individua due sottospazi:
\begin{enumerate}
    \item $\image{L} = \{ v' \in V' : \exists v \in V \text{ t.c. } L(v) = v' \} \le V'$
    \item $\ker L = \{ v \in V : L(v) = \nullelement_{V'} \} \le V$
\end{enumerate}

\subsection{Teorema di omomorfismo per gli spazi vettoriali}

\begin{theorem}
Data un'applicazione lineare $L : (V, +, \cdot) \to (V', +, \cdot)$ sul campo $\field$, si ha che:
\begin{enumerate}
    \item $\ker L \le V$
    \item $\image{L} \le V'$
    \item $V / \ker L \cong \image{L}$, ossia i due insiemi sono isomorfi
\end{enumerate}
\end{theorem}
Alcune propriet\`a delle applicazioni lineari:
\begin{enumerate}
    \item $L(\nullelement_V) = \nullelement_{V'}$
    \item $L(-v) = - L(v)$
    \item $L(a \cdot v + b \cdot w) = a \cdot L(v) + b \cdot L(w)$
    \item $L^{-1} (v') = v + \ker L$, ossia tutti i vettori $v$ tali per cui $L(v) = v'$ si ottengono sommando $v$ con gli elementi del nucleo
    \item\label{itm:morfismo_dipendenza} $L$ manda insiemi dipendenti in insiemi dipendenti. Ossia, dato $S \le V$, se \`e dipendente in $V$, $L(S)$ \`e dipendente in $V'$
\end{enumerate}
\begin{proof}[della propriet\`a \ref{itm:morfismo_dipendenza}]
$S \le V$ \`e dipendente, ossia $\nullelement_V = a_1 \cdot s_1 + \dots + a_n \cdot s_n$ dove $s_i \in S$ e almeno un $a_i \neq 0$. Quindi:
\[
L(\nullelement_V) = L(a_1 \cdot s_1 + \dots + a_n \cdot s_n) = a_1 \cdot L(s_1) + \dots + a_n \cdot L(s_n) = \nullelement_{V'}
\]
Quindi anche $L(S)$ \`e dipendente.
\end{proof}
Prendendo un insieme indipendente, non sappiamo con certezza cosa succede, ma possiamo dire quanto segue:
\begin{prop}
Un'applicazione lineare iniettiva $L : V \to V'$, ossia tale che $\ker L = \{ \nullelement_{V} \}$, manda insiemi indipendenti in insiemi indipendenti, e viceversa un'applicazione che manda insiemi indipendenti in insiemi indipendenti \`e un'applicazione iniettiva.
\end{prop}
\begin{proof}
Vediamo che \`e condizione necessaria. L'applicazione $L$ \`e iniettiva. Prendiamo $S$ indipendente, come tesi abbiamo che $L(S)$ \`e a sua volta indipendente. Prendiamo la combinazione lineare $\nullelement_V = a_1 \cdot s_1 + \dots + a_n \cdot s_n$, sappiamo che ogni $a_i = 0$, abbiamo che $L(\nullelement_V) = \nullelement_{V'} = L \left( a_1 \cdot s_1 + \dots + a_n \cdot s_n \right) = a_1 \cdot L(s_1) + \dots + a_n \cdot L(s_n)$, e tutti i coefficienti $a_i$ sono uguali a 0.

Vediamo che \`e condizione sufficiente. Se $S$ \`e indipendente, la sua immagine $L(S)$ \`e indipendente. Se prendiamo $v \in \ker L$ e supponiamo per assurdo che $v \neq \nullelement_V$, ma che, essendo nel $\ker L$, $L(v) = \nullelement_{V'}$, abbiamo che l'immagine dello spazio indipendente $S = \{ v \}$ \`e $L(S) = \{ \nullelement_{V'} \}$ che non \`e indipendente. Quindi $v$ deve essere il vettore nullo.

Possiamo anche vedere che $L(v) = L(w) \implies v = w$. Sempre sotto l'ipotesi che insiemi indipendenti vanno in insiemi indipendenti, $L(v - w) = \nullelement_{V'}$, quindi il vettore $v - w$ \`e nel $\ker L$. $v - w$ deve essere uguale al vettore nullo, e quindi $v$ deve  essere uguale a $w$, per lo stesso motivo di sopra.
\end{proof}

\begin{exmp}
$L : \reals^3 \to \reals^2$, definita come $L(a,b,c) = (a+1, b+c)$ non \`e un'applicazione lineare. Infatti il vettore nullo $(0,0,0)$ va in $(1,0)$.

$L: \reals^3 \to \reals^2$ definita come $L(a,b,c) = (a + b, a + c)$. Vediamo se \`e un'applicazione lineare.
\begin{align*}
L \left( (a,b,c) + (a',b',c') \right) = (a+b,a+c) + (a' + b', a' + c') = \\ = (a + a' + b + b', a + a' + c + c') = L \left( a + a', b + b', c + c' \right)
\end{align*}
Controlliamo se conserva anche il prodotto scalare. Prendiamo un $k \in \reals$.
\[
L \left( k \cdot (a,b,c) \right) = L \left( k \cdot a, k \cdot b, k \cdot c \right) = (k \cdot a + k \cdot b, k \cdot a + k \cdot c) = k \cdot (a + b, a + c) = k \cdot L(a, b, c)
\]
Quindi questa \`e un'applicazione lineare. Qual \`e il nucleo?
\begin{align*}
\ker L &= \{ (a,b,c) \in \reals^3 : L(a,b,c) = (a + b, a +c) = (0, 0)\} = \\
&= \{ (a, b, c) \in \reals^3 : a + b = 0 \text{ e } a + c = 0 \} = \\ 
&= \{ (a,b,c) \in \reals^3 : a = -b = -c \} = \\
&= \{ (a, -a, -a) \in \reals^3 : a \in \reals \}
\end{align*}
Il nucleo ha dimensione 1. Il nucleo infatti \`e isomorfo a $\reals$, o equivalentemente \`e ottenuto da tutti i multipli di $(1, -1, -1)$. Quindi l'applicazione non \`e iniettiva.

Come \`e fatta l'immagine?
\[
\image{L} = \{ (x, y) : L(a,b,c) = (x,y) \} = \reals^2
\]
Infatti posso prendere la terna $(0, x, y)$, ho che la sua immagine \`e $(x,y)$. Quindi l'immagine ha dimensione 2.

Possiamo notare che $\dim \reals^3 = \dim \ker L + \dim \image{L}$. La dimensione del dominio \`e data dalla dimensione del nucleo pi\`u la dimensione dell'immagine.

L'applicazione $L(a,b,c) = (a^2, b+c)$ non \`e lineare. Si vede subito, perch\'e c'\`e un quadrato che causa rogne.

$a,b,c$ sono le coordinate del vettore rispetto alla base canonica $\{ (1, 0, 0), (0, 1, 0), (0, 0, 1) \}$. Per avere un'applicazione lineare, le coordinate dell'immagine devono essere date da equazioni lineari.

Cambiamo dominio. $L : \matrices_2 (\reals) \to \reals^3$.
\[
L \left( 
\begin{smallpmatrix}
a & b \\
c & d
\end{smallpmatrix}
\right) = ( a + 2b, d, a +d)
\]
Il suo nucleo \`e:
\[
\ker L = \{ 
\begin{smallpmatrix}
a & b \\
c & d
\end{smallpmatrix}
: L \left(
\begin{smallpmatrix}
a & b \\
c & d
\end{smallpmatrix}
\right) = (0, 0, 0) \}
\]
Ossia tutte le matrici tali che $a + 2b = 0$, $d = 0$, $a + d = 0$. Quindi $a, b, d$ sono tutti 0.
\[
\ker L = \left\{ 
\begin{smallpmatrix}
a & b \\
c & d
\end{smallpmatrix}
: d = a = b = 0 \right\} = \left\{ 
\begin{smallpmatrix}
0 & 0 \\
c & 0
\end{smallpmatrix}
: c \in \reals \right\}
\]
Il $\ker L$ ha dimensione 1, essendo ottenuto interamente da $\begin{smallpmatrix} 0 & 0 \\ 1 & 0 \end{smallpmatrix}$. Quindi l'immagine \`e tutto $\reals^3$, dovendo avere dimensione 3.

Consideriamo l'applicazione $L( \begin{smallpmatrix} a & b \\ c & d \end{smallpmatrix} ) = (a + d, d, a +d)$, abbiamo che il nucleo \`e:
\[
\ker L = \left\{
\begin{smallpmatrix}
0 & b \\
c & 0
\end{smallpmatrix}
: b, c \in \reals \right\}
\]
In questo caso il nucleo ha dimensione 2, e si pu\`o ottenere a partire dai vettori $\begin{smallpmatrix} 0 & 1 \\ 0 & 0 \end{smallpmatrix}$ e $\begin{smallpmatrix} 0 & 0 \\ 1 & 0 \end{smallpmatrix}$. Ora l'immagine ha dimensione 2. Come \`e fatta l'immagine?
\[
\image{L} = \{ (x, y, z) \in \reals^3 : x = a + d, y = d, z = a + d \} = 
\{ (x, y, x) \in \reals^3 : x = a + d, y = d \}
\]
L'immagine \`e generata dai vettori $(1,0,1)$ e $(0,1,0)$.
\end{exmp}
\begin{prop}
Dati $v_1', \ldots v_n'$ vettori di $V'$ e una base $B = \{ b_1, \dots b_n \}$ di $V$, esiste una sola applicazione lineare $L : V \to V'$ tale che $L(b_i) = v_i'$. Non sappiamo niente sulle dimensioni di $V$ e di $V'$, ossia non sono necessariamente uguali.
\end{prop}
\begin{proof}
Consideriamo il vettore $v = a_1 \cdot b_1 + \ldots + a_n \cdot b_n$. La sua immagine \`e:
\[
L(v) = L(a_1 \cdot b_1 + \ldots + a_n \cdot b_n) = a_1 \cdot v_1' + \ldots + a_n \cdot v_n'
\]
$L$ \`e l'applicazione lineare cercata, ed \`e unica. Sia $L'$ tale che $L'(b_i) = v_i'$, allora $L = L'$.
\[
L(V) = a_1 \cdot v_1' + \ldots + a_n \cdot v_n' = a_1 \cdot L'(b_1) + \ldots + a_n \cdot L'(b_n) =  L' (a_1 \cdot b_1 + \ldots + a_n \cdot b_n)
\]
Le applicazioni quindi sono uguali, perch\'e assumono gli stessi valori su ogni vettore.
\end{proof}
\subsection{Basi e applicazioni lineari}
Per definire un'applicazione lineare basta fornire i valori che l'applicazione fornisce per la base. Consideriamo ad esempio $L : \matrices_2 (\reals) \to \reals^5$.
\begin{gather*}
\begin{pmatrix}
1 & 0 \\
0 & 0
\end{pmatrix}
\to (2, 0, 0, 1, 0) \\
\begin{pmatrix}
0 & 1 \\
0 & 0
\end{pmatrix}
\to (0, 0, 0, 0, 0) \\
\begin{pmatrix}
0 & 0 \\
1 & 0
\end{pmatrix}
\to (1, 1, 1, 1, 1) \\
\begin{pmatrix}
0 & 0 \\
0 & 1
\end{pmatrix}
\to (0, 1, 0, 1, 0)
\end{gather*}
Come si trova l'immagine di un vettore qualsiasi?
\begin{align*}
L \left(
\begin{smallpmatrix}
a & b \\
c & d
\end{smallpmatrix}
\right) &= L \left( a \cdot 
\begin{smallpmatrix}
1 & 0 \\
0 & 0
\end{smallpmatrix}
+ b \cdot 
\begin{smallpmatrix}
0 & 1 \\
0 & 0
\end{smallpmatrix}
+ c \cdot 
\begin{smallpmatrix}
0 & 0 \\
1 & 0
\end{smallpmatrix}
+ d \cdot 
\begin{smallpmatrix}
0 & 0 \\
0 & 1
\end{smallpmatrix}
\right) = \\
&= a \cdot L \left(
\begin{smallpmatrix}
1 & 0 \\
0 & 0
\end{smallpmatrix}
\right) + b \cdot L \left(
\begin{smallpmatrix}
0 & 1 \\
0 & 0
\end{smallpmatrix}
\right) + c \cdot L \left(
\begin{smallpmatrix}
0 & 0 \\
1 & 0
\end{smallpmatrix}
\right) + d \cdot L \left(
\begin{smallpmatrix}
0 & 0 \\
0 & 1
\end{smallpmatrix}
\right) = \\
&= a \cdot (2, 0, 0, 1, 0) + b \cdot (0, 0, 0, 0, 0) + c \cdot (1, 1, 1, 1, 1) + d \cdot (0, 1, 0, 1, 0) = \\
&=(2a + c, c + d, c, a + c + d, c)
\end{align*}
\subsection{Isomorfismi fra spazi vettoriali}
Un'applicazione lineare biunivoca si dice ``isomorfismo''. Gli spazi vettoriali si dicono isomorfi. Spazi vettoriali isomorfi hanno la stessa dimensione. Vuol dire che esiste un'applicazione lineare $L : V \to V'$ isomorfa fra i due spazi vettoriali, e che quindi ogni base di $V$ ha per immagine una base di $V'$.
\begin{theorem}[Teorema di isomorfismo]
$V$ \`e uno spazio vettoriale su $\field$ con $\dim V = n$, allora $V$ \`e isomorfo a $\field^n$, e viceversa, se $V$ \`e isomorfo a $\field^n$, allora $\dim V = n$.
\end{theorem}
\begin{proof}
Per ipotesi, $V$ \`e uno spazio vettoriale su $\field$ di dimensione $\dim V = n$. Qual \`e l'isomorfismo con $\field^n$?
\[
L_B : \field^n \to V 
\]
Abbiamo tante applicazioni isomorfe $L_B$, a seconda della base $B$ di $V$ che fissiamo. Siccome $\dim V = n$, $\abs{B} = n$. Sia $B$ una base $\{ e_1, \ldots, e_n \}$:
\[
L_B (a_1, \ldots, a_n) = v = a_1 \cdot e_1 + \dots + a_n \cdot e_n
\]
$L_B$ associa ad ogni $n$-upla ($a_{1}, \ldots, a_{n}$) il vettore di coordinate $a_{1}, \ldots, a_{n}$ rispetto alla base $B$. Infatti:
\begin{align*}
L_B \left( (a_{1}, \ldots, a_{n}) + (b_{1}, \ldots, b_{n}) \right) = \\
= L_B \left( a_1 + b_1, \ldots,  a_n + b_n \right) = \\
= (a_1 + b_1) \cdot e_1 + \ldots + (a_n + b_n) \cdot e_n = \\
= (a_1 \cdot e_1 + \ldots + a_n \cdot e_n) + (b_1 \cdot e_1 + \ldots + b_n \cdot e_n) = \\
= L_B (a_{1}, \ldots, a_{n}) + L_B(b_{1}, \ldots, b_{n})
\end{align*}
Banalmente conserva anche il prodotto per uno scalare
\[
L_B(k \cdot (a_{1}, \ldots, a_{n})) = k \cdot L_B(a_{1}, \ldots, a_{n})
\]
$\ker L_B = \{ \nullelement \}$, poich\'e se $L_B(a_{1}, \ldots, a_{n}) = \nullelement$, allora $a_1 \cdot e_1 + \ldots + a_n \cdot e_n = \nullelement$, ma essendo $B$ una base l'unica combinazione lineare che d\`a il vettore nullo \`e quella banale, quindi $a_i = 0$.
\end{proof}

Un esempio tipico sono i vettori geometrici del piano. Ad un vettore nel piano corrisponde una coppia di punti, che non sono altro che le coordinate del vettore rispetto alla base $B = \{ i, j \}$ con $\abs{i} = 1$ e $\abs{j} = 1$.

Possiamo studiare solo le $n$-uple di elementi di un campo $\field$ come spazi vettoriali, e da quelle passare a tutti gli altri spazi.

Vediamo un'ultima propriet\`a delle applicazioni lineari. Sia $L : V \to V'$ un'applicazione lineare, e sia $\dim V = n$ (finito):
\[
\dim V = \dim \ker L + \dim \image{L}
\]
\begin{proof}
\[
\ker L 
\begin{cases}
= \{ \nullelement \} \implies L \text{ \`e iniettiva, e } \bar L : V \to \image{L} \text{ \`e suriettiva} \implies \dim V = \dim \image{L} \\
\neq \{ \nullelement \}
\end{cases}
\]
Nel secondo caso, $B_k = \{ u_{1}, \ldots, u_{t} \}$ \`e una base del $\ker L$, quindi $\dim \ker L = t \implies$ per il teorema del complemento esiste $H$ sottoinsieme di $V$, $H = \{ {w}_{t+1}, \ldots, {w}_{n} \}$ tale che $B_k \cup H$ \`e una base di $V \implies L(H)$ \`e una base di $\image{L}$, quindi $\dim \image{L} = n - t$.

Bisogna dimostrare che $\pow{L(H)} = \image{L}$, e che $L(H)$ \`e indipendente.

Prendiamo un vettore $v' \in \image{L} $ tale che $ L(v) = v'$. 
\begin{align*}
v' = L(v) &= L(a_1 \cdot u_1 + \ldots + a_t \cdot u_t + a_{t+1} \cdot w_{t+1} + \ldots + a_{n} \cdot w_{n}) = \\
&= \underbrace{a_1 \cdot L(u_1) + \ldots + a_t \cdot L(u_t)}_{\nullelement} + a_{t+1} \cdot L(w_{t+1}) + \ldots + a_n \cdot L(w_n)
\end{align*}
Quindi qualsiasi elemento di $\image{L}$ si pu\`o scrivere come combinazione di vettori di $\pow{L(H)}$. Vediamo ora che \`e indipendente. Prendiamo una combinazione lineare che d\`a il vettore nullo, e mostriamo che \`e banale:
\begin{align*}
\nullelement &= a_{t+1} \cdot L(w_{t+1}) + \ldots + a_n \cdot L(w_n) = \tag{essendo $L$ lineare} \\ 
&= L \left( a_{t+1} \cdot w_{t+1} + \ldots + a_n \cdot w_n \right)
\end{align*}
Quindi il vettore $v = a_{t+1} \cdot w_{t+1} + \ldots + a_n \cdot w_n \in \ker L$, quindi $v = b_1 \cdot u_1 + \dots b_t \cdot u_t$. Quindi, ancora, $\nullelement = a_{t+1} \cdot w_{t+1} + \ldots + a_n \cdot w_n - (b_1 \cdot u_1 + \ldots b_t \cdot u_t)$, ossia una combinazione lineare di vettori di $B_k \cup H$, che \`e una base di $V$, quindi tutti i coefficienti $a_i$ sono uguali a 0.
\end{proof}

\subsection{Analogia tra cardinalit\`a e dimensione}

Sia $\Gamma$ un insieme di cardinalit\`a $\abs{\Gamma} = n$, e sia $V$ uno spazio vettoriale su $\field$ di dimensione $\dim V = n$. $(\parts(\Gamma), \subseteq)$ \`e un reticolo. Anche $(\subgroupset(V), \subseteq)$ \`e un reticolo. L'$\inf$ nel primo \`e l'intersezione, il $\sup$ \`e l'unione.

Nel secondo caso l'$\inf$ di due sottospazio \`e $W \cap U = W \infop U$, mentre il $\sup$ \`e $W + U = W \supop U$.

La cardinalit\`a di un sottoinsieme $A \subseteq \Gamma$ \`e $\abs{\emptyset} = 0 \le \abs{A} \le n = \abs{\Gamma}$.

Negli spazi vettoriali, $\dim \{ \nullelement\} = 0 \le \dim W \le n = \dim V$.

L'unico sottospazio di dimensione $n$ \`e lo spazio stesso.

Un'applicazione qualunque, ossia un morfismo di insiemi, abbiamo la stessa analogia con gli spazi vettoriali. Consideriamo un altro insieme $\Gamma'$ con cardinalit\`a $\abs{\Gamma} = \abs{\Gamma'}$. Consideriamo l'applicazione $f : \Gamma \to \Gamma'$. In questo caso $f$ \`e iniettiva $\iff f$ \`e suriettiva. Stessa cosa vale con le applicazioni lineari fra spazi vettoriali con la stessa dimensione, ossia $L : V \to V'$ con $\dim V = \dim V'$. $L$ \`e iniettiva $\iff L$ \`e suriettiva, per la formula vista prima. 

Nel primo caso $\dim V = \dim \ker L + \dim \image{L}$, essendo iniettiva $\dim \ker L = 0$, quindi $\dim \image{L} = \dim V = \dim V'$. Viceversa, se \`e suriettiva $\dim \image{L} = \dim V' = n$, quindi $\dim \ker L$ necessariamente \`e 0.

Se $\abs{\Gamma'} > \abs{\Gamma}$, non esistono funzioni suriettive, e al contrario se $\abs{\Gamma'} < \abs{\Gamma}$ non esistono funzioni iniettive. Vale lo stesso con gli spazi vettoriali: se $\dim V < \dim V'$, non esistono applicazioni lineari iniettive $L : V \to V'$, e se invece $\dim V > \dim V'$ non esistono applicazioni lineari suriettive $L : V \to V'$.

Nel primo caso $n = \dim V < \dim \ker L + \dim \image{L}$, con $\dim \image{L} = m > n$. La dimensione \`e sempre un numero positivo. Nel secondo caso:
\[
n = \dim V > \underbrace{\dim \ker L}_{0} + \dim \image{L}, \text{ con } \dim \image{L} = m < n
\]
La cardinalit\`a del $\sup$ di due insiemi \`e $\abs{A \cup B} = \abs{A} + \abs{B} - \abs{A \cap B}$, che vista dal punto di vista del reticolo \`e $\abs{A \supop B} = \abs{A} + \abs{B} - \abs{A \infop B}$. Negli spazi vettoriali, invece vediamo che:
\[
\dim (W + U) = \dim U + \dim W - \dim (U \cap W)
\]
Questa sopra \`e detta formula di Grassmann.

L'applicazione che a un sottoinsieme $A$ di $\Gamma$ associa la sua cardinalit\`a, $A \mapsto \abs{A}$, \`e un'applicazione $\parts(\Gamma) \to \naturals$, \`e analoga all'applicazione che a un sottospazio $W$ di $V$ associa la sua dimensione, $W \mapsto \dim W$, ossia $\subgroupset(V) \to \naturals$.

Nell'insieme delle parti, l'applicazione conta il numero di elementi massimale che va da $\emptyset$ a $A$ diminuito di $1$, ossia parte dall'insieme vuoto e aggiunge un elemento:
\[
\emptyset \subset \{ 1 \} \subset \{ 1, 2 \} \subset \{ 1, 2, 3 \} \subset A
\]
Con $A = \{ 1, 2, 3, 4 \}$.

Allo stesso modo si pu\`o costruire una catena massimale che aggiunge un vettore alla volta fino ad ottenere la base di un sottospazio.
\[
\{ \nullelement \} \subset \{ e_1 \} \subset \dots \subset \dim W
\]
Questi sono reticoli dotati di funzioni ``rango'', che vanno dal reticolo in $\naturals$, e che contano le catene. Ossia associano a ogni elemento del reticolo la cardinalit\`a della catena massimale diminuita di 1.

\begin{proof}[della formula di Grassmann]
\[
\dim(U + W) = \dim U + \dim W - \dim (U \cap W)
\]
$U + W$ \`e definito come:
\[
U + W = \sum (U \cup W) = \{ v = u + w : u \in U, w \in W \}
\]
Se $U \cap W = \nullelement \implies (B_U \cup B_W)$ \`e una base di $U + W$.

Altrimenti, se $U \cap W \neq \nullelement$, con $\dim U = h$ e $\dim W = k$, sia $B = \{ e_{1}, \ldots, e_{t} \}$ una base di $U \cap W$. Prendiamo una base di $U$, $B_U \supset B$, $B_U = \{ e_{1}, \ldots, e_{t} , u_{t+1}, \ldots, u_{h} \}$, e allo stesso modo prendiamo una base di $W$, $B_W \supset B$, $B_W = \{ e_{1}, \ldots, e_{t}, w_{t+1}, \ldots, w_{k} \}$. Come \`e la base della somma? Deve avere dimensione $h + k - t$. Dimostriamo quindi che $B_U \cup \{ w_{t+1}, \ldots, w_{k} \}$ \`e una base di $U + W$.

Che sia un sistema di generatori \`e banale: ogni vettore di $U$ si scrive come combinazione lineare di vettori di $B_U$, e ogni vettore di $W$ si scrive come combinazione lineare di vettori di $B_U \cup \{ w_{t+1}, \ldots, w_{k} \}$, essendo gli elementi di $B$ contenuti in $B_U$.

Vediamo che \`e proprio una base, quindi \`e indipendente.
\[
\nullelement = \underbrace{a_1 \cdot e_1 + \ldots + a_t \cdot e_t + a_{t+1} \cdot u_{t+1} + \ldots + a_h \cdot u_h}_{\in U} + \underbrace{b_{t+1} \cdot w_{t+1} + \ldots + b_k \cdot w_k}_{\in W}
\]
Quindi:
\[
v = a_1 \cdot e_1 + \ldots + a_t \cdot e_t + a_{t+1} \cdot u_{t+1} + \ldots a_h \cdot u_h = -(b_{t+1} \cdot w_{t+1} + \ldots + b_k \cdot w_k) \in U \cap W
\]
Appartenendo all'intersezione, possiamo scriverlo come combinazione lineare di elementi della base dell'intersezione:
\[
v = c_1 \cdot e_1 + \ldots + c_t \cdot e_t =  - (b_{t+1} \cdot w_{t+1} + \ldots + b_k \cdot w_k)
\]
Quindi:
\[
\nullelement = c_1 \cdot e_1 + \ldots + c_t \cdot e_t + b_{t+1} \cdot w_{t+1} + \ldots + b_k \cdot w_k
\]
Tutti i coeficienti quindi sono 0 e tutti i vettori sono indipendenti.
\end{proof}

\section{Rappresentazione di applicazioni lineari (con matrici)}
Consideriamo la matrice seguente.
\[
A =
\begin{pmatrix}
1 & 0 & 2 & 1 \\
1 & 1 & 0 & 0 \\
0 & 1 & 1 & 1
\end{pmatrix}
\in \matrices_{3,4} (\reals)
\]
La matrice $A$ individua un'applicazione lineare $L_A : \reals^4 \to \reals^3$.
\[
L_A \left(
\begin{smallpmatrix}
x \\
y \\ 
z \\
t
\end{smallpmatrix}
\right) = A \times 
\begin{pmatrix}
x \\
y \\ 
z \\
t
\end{pmatrix}
\]
Per il prodotto fra matrici, $A_{3,4} \times X_{4,1} = B_{3,1}$.
\[
\begin{pmatrix}
1 & 0 & 2 & 1 \\
1 & 1 & 0 & 0 \\
0 & 1 & 1 & 1
\end{pmatrix}
\times 
\begin{pmatrix}
x \\
y \\ 
z \\
t
\end{pmatrix}
=
\begin{pmatrix}
1 \cdot x + 0 \cdot y + 2 \cdot z + 1 \cdot t \\
1 \cdot x + 1 \cdot y + 0 \cdot z + 0 \cdot t \\
0 \cdot x + 1 \cdot y + 1 \cdot z + 1 \cdot t
\end{pmatrix}
\]
Quindi, ad esempio:
\[
L \left(
\begin{smallpmatrix}
1 \\
1 \\
0 \\
0
\end{smallpmatrix}
\right) = 
\begin{pmatrix}
1 \\
2 \\
1
\end{pmatrix}
\]
Si vede subito che questa applicazione \`e un'applicazione lineare, per le propriet\`a del prodotto tra matrici.
\begin{align*}
L_A(X+Y) = A \times (X + Y) = A \times X + A \times Y \\
L_A(k \cdot X) = A \times (k \cdot X) = k \cdot A \times X = k \cdot L_A (X)
\end{align*}
Dove vanno a finire i vettori della base canonica?
\begin{align*}
L_A \left(
\begin{smallpmatrix}
1 \\ 0 \\ 0 \\ 0
\end{smallpmatrix}
\right) &= 
\begin{pmatrix}
1 \\ 1 \\ 0
\end{pmatrix} = A^1 \tag{la prima colonna di $A$} \\
L_A \left(
\begin{smallpmatrix}
0 \\ 1 \\ 0 \\ 0
\end{smallpmatrix}
\right) &= 
\begin{pmatrix}
0 \\ 1 \\ 1
\end{pmatrix} = A^2 \tag{la seconda colonna di $A$} \\
L_A \left(
\begin{smallpmatrix}
0 \\ 0 \\ 1 \\ 0
\end{smallpmatrix}
\right) &= 
\begin{pmatrix}
2 \\ 0 \\ 1
\end{pmatrix} = A^3 \tag{la terza colonna di $A$} \\
L_A \left(
\begin{smallpmatrix}
0 \\ 0 \\ 0 \\ 1
\end{smallpmatrix}
\right) &= 
\begin{pmatrix}
1 \\ 0 \\ 1
\end{pmatrix} = A^4 \tag{la quarta colonna di $A$}
\end{align*}
In generale $L_A (e_n) = A^n$, ossia l'$n$-esimo vettore della base canonica mi d\`a la colonna $n$-esima. Quindi:
\[
L_A(X) = A \times X = A^1 \cdot x + A^2 \cdot y + A^3 \cdot z + A^4 \cdot t
\]
Se in generale moltiplico per una matrice colonna con $n$ elementi ($x_{1}, \ldots, x_{n}$):
\[
L_A(X) = A \times X = A^1 \cdot x_1 + \dots + A^n \cdot x_n
\]
Funziona anche al contrario, ossia \`e possibile passare da un'applicazione lineare ad una matrice. Consideriamo la seguente applicazione lineare $L : \reals^3 \to \reals^2$:
\[
L \left(
\begin{smallpmatrix}
x \\ y \\ z
\end{smallpmatrix}
\right) = 
\begin{pmatrix}
2x+y \\ z
\end{pmatrix}
\]
Quest'applicazione \`e un'applicazione $L_A$ individuata da una matrice:
\[
A = 
\begin{pmatrix}
2 & 1 & 0 \\
0 & 0 & 1
\end{pmatrix}
\]
Le colonne sono i valori che la matrice assume nei vettori della base canonica.
\begin{align*}
L \left(
\begin{smallpmatrix}
1 \\ 0 \\ 0
\end{smallpmatrix}
\right) = 
\begin{pmatrix}
2 \\ 0
\end{pmatrix} \\
L \left(
\begin{smallpmatrix}
0 \\ 1 \\ 0
\end{smallpmatrix}
\right) = 
\begin{pmatrix}
1 \\ 0
\end{pmatrix} \\
L \left(
\begin{smallpmatrix}
0 \\ 0 \\ 1
\end{smallpmatrix}
\right) = 
\begin{pmatrix}
0 \\ 1
\end{pmatrix}
\end{align*}
Quindi ogni matrice $A_{m \times n} \in \matrices_{m \times n} (\field)$ individua un'applicazione lineare $L_A : \field^n \to \field^m$, definita come:
\[
X = 
\begin{pmatrix}
x_1 \\ \vdots \\ x_n
\end{pmatrix}
\qquad
L_A (X) = A \times X = A^1 \cdot x_1 + \ldots + A^n \cdot x_n
\]
E viceversa ogni applicazione lineare $L : \field^n \to \field^m$ individua una matrice $A$ in cui l'$i$-esima colonna $A^i = L(e_i)$, con $e_i$ l'$i$-esimo elemento della base canonica di $\field^n$.

Una matrice $A_{m \times n}$ individua $m$ righe $A_1 \dots A_m$, e ciascuna riga \`e un elemento di $\field^n$. Identicamente ciascuna delle $n$ colonne $A^1 \dots A^n$ \`e un elemento di $\field^m$.

Lo spazio generato dalle colonne, $\pow{A^1 \dots A^n}$ \`e un sottospazio di $\field^m$, mentre lo spazio generato dalle righe $\pow{A_1 \dots A_m}$ \`e un sottospazio di $\field^n$.

Lo spazio generato dalle colonne della matrice \`e l'immagine dell'applicazione $L_A$.
\[
\pow{A^1 \dots A^n} = \image{L_A}
\]
Si pu\`o prendere anche una base qualunque $B$, non obbligatoriamente una base canonica.

Riprendiamo l'applicazione $L : \reals^3 \to \reals^2$ di prima.
\[
L \left(
\begin{smallpmatrix}
x \\ y \\ z
\end{smallpmatrix}
\right) = 
\begin{pmatrix}
2x+y \\ z
\end{pmatrix}
\]
La matrice associata alla base canonica \`e:
\[
A = 
\begin{pmatrix}
2 & 1 & 0 \\
0 & 0 & 1
\end{pmatrix}
\]
Possiamo associarci un'altra matrice cambiando base. Ad esempio, prendiamo la base di $\reals^3$ $B = \{ (1,1,0), (0,0,1), (0,1,1) \}$. Possiamo calcolare $L$ nei vettori della base, e ottenere una nuova matrice:
\begin{align*}
L \left(
\begin{smallpmatrix}
1 \\ 1 \\ 0
\end{smallpmatrix}
\right) = 
\begin{pmatrix}
3 \\ 0
\end{pmatrix} \\
L \left(
\begin{smallpmatrix}
0 \\ 0 \\ 1
\end{smallpmatrix}
\right) = 
\begin{pmatrix}
0 \\ 1
\end{pmatrix} \\
L \left(
\begin{smallpmatrix}
0 \\ 1 \\ 1
\end{smallpmatrix}
\right) = 
\begin{pmatrix}
1 \\ 1
\end{pmatrix}
\end{align*}
\[
A_B = 
\begin{pmatrix}
3 & 0 & 1 \\
0 & 1 & 1
\end{pmatrix}
\]
% % questo teorema e' una ripetizione
% \begin{prop}
% Data una base $B$ di $V$, con $\dim B = n$, ossia $B = \{ e_{1}, \ldots, e_{n} \}$, e un altro spazio vettoriale $V'$, e $n$ vettori ${v'}_{1}, \ldots, {v'}_{n} \in V'$, esiste una sola applicazione lineare $L : V \to V'$ tale che $L(e_i) = v_i'$.
% \end{prop}

\begin{exmp}
Consideriamo gli spazi vettoriali $V = \reals_3[x]$ e $V' = \matrices_2 (\reals)$, e la base canonica $B_c = \{ 1, x, x^2, x^3 \}$ di $V$. L'applicazione \`e definita come:
\begin{gather*}
L(1) = 
\begin{smallpmatrix}
1 & 1 \\ 0 & 0
\end{smallpmatrix} \\
L(x) = 
\begin{smallpmatrix}
0 & 0 \\ 1 & 1
\end{smallpmatrix} \\
L(x^2) = 
\begin{smallpmatrix}
0 & 0 \\ 0 & 0
\end{smallpmatrix} \\
L(x^3) = 
\begin{smallpmatrix}
1 & 1 \\ 1 & 1
\end{smallpmatrix}
\end{gather*}
L'immagine di un generico polinomio $p(x)$ \`e:
\begin{align*}
L(p(x)) &= L(a_0 + a_1 \cdot x + a_2 \cdot x^2 + a_3 \cdot x^3) = \\
&= a_0 \cdot L(1) + a_1 \cdot L(x) + a_2 \cdot L(x^2) + a_3 \cdot L(x^3) = \\
&= a_0 \cdot 
\begin{smallpmatrix}
1 & 1 \\ 0 & 0
\end{smallpmatrix}
+ a_1 \cdot 
\begin{smallpmatrix}
0 & 0 \\ 1 & 1
\end{smallpmatrix}
+ a_2 \cdot 
\begin{smallpmatrix}
0 & 0 \\ 0 & 0
\end{smallpmatrix} 
+ a^3 \cdot 
\begin{smallpmatrix}
1 & 1 \\ 1 & 1
\end{smallpmatrix} = \\
&=
\begin{pmatrix}
a_0+a_3 & a_0+a_3 \\
a_1+a_3 & a_1+a_3
\end{pmatrix}
\end{align*}
Cambiamo base. Possiamo prendere un'altra base semplicemente cambiando l'ordine, ossia considerare $B = \{ x, x^3, 1, x^2 \}$. Le coordinate di un generico vettore rispetto alla base canonica sono $(a_0, a_1, a_2, a_3)$. Rispetto alla nuova base, le coordinate sono $(a_1, a_3, a_0, a_2)$. L'applicazione \`e identica, ma stavolta va scritta come:
\[
L(a_1, a_3, a_0, a_2) = a_1 \cdot L(x) + a_3 \cdot L(x^3) + a_0 \cdot L(1) + a_2 \cdot L(x^2)
\]
\end{exmp}
Le basi sono insiemi ordinati. Alle $n$ coordinate $(x_{1}, \ldots, x_{n})$ devo sapere quale elemento della base associare.

Ad una matrice $A_{m \times n} \in \matrices_{m \times n} ( \field )$, con in genere $\field = \reals$, possiamo associare un'applicazione lineare $L_A : \field^n \to \field^m$ tale che:
\[
L_A(X) = A_{m \times n} \times X_{n \times 1} = B_{m \times 1} \in \field^n \text{ con } X = 
\begin{pmatrix}
x_1 \\ \vdots \\ x_n
\end{pmatrix}
\]
$L_A$ \`e lineare per le propriet\`a del prodotto tra matrici.

Abbiamo visto che:
\[
L_A(X) = A \times X = A^1 \cdot x_1 + \ldots + A^n \cdot x_n
\]
Data una base canonica $B_c = \{ e_1, \ldots, e_n \}$, l'immagine dell'$i$-esimo elemento $L(e_i) = A^i$ \`e l'$i$-esima colonna.

Viceversa data un'applicazione lineare $L : \field^n \to \field^m$, esiste un'unica matrice $A$ tale che $L = L_A$, ed \`e l'unica matrice le cui colonne sono le coordinate delle immagini dei vettori della base canonica di $\field^n$.
\[
A^i = L(e_i)
\]
Vediamo come funziona su campi $\field$ qualsiasi. Consideriamo un'applicazione lineare $L : V \to V'$, e due basi $B$ e $B'$, rispettivamente di $V$ e di $V'$. $B = \{ e_{1}, \ldots, e_{n} \} \implies \dim V = n$, e $B' = \{ {e'}_{1}, \ldots, {e'}_{m} \} \implies \dim V' = m$. L'immagine di un generico $v \in V$ \`e:
\[
L(v) = L(x_1 \cdot e_1 + \ldots + x_n \cdot e_n)
\]
$(x_{1}, \ldots, x_{n})$ \`e la $n$-upla delle coordinate rispetto a $B$. Il vettore $v$ si pu\`o anche scrivere come prodotto $B \times X$:
\[
v = 
\begin{pmatrix}
e_1 & \dots & e_n
\end{pmatrix} 
\times 
\begin{pmatrix}
x_1 \\ \vdots \\ x_n 
\end{pmatrix}
= x_1 \cdot e_1 + \ldots + x_n \cdot e_n
\]
Anche un vettore $v' \in V'$ si pu\`o scrivere come la sua base per le sue coordinate, ossia $v' = B' \times X'$.

Le immagini degli elementi della base canonica saranno:
\begin{align*}
L(e_1) &= a_{1,1} \cdot {e'}_{1} + \ldots + a_{m,1} \cdot {e'}_{m} = B' \times A^1 \\
\vdots & \\
L(e_n) &= a_{1,n} \cdot {e'}_{1} + \ldots + a_{m,n} \cdot {e'}_{m} = B' \times A^n
\end{align*}
La matrice associata a $L$ \`e la matrice $A$ che ha per colonne le coordinate delle immagini dei vettori della base $B$ di $V$ rispetto alla base $B'$ di $V'$. Tornando all'esempio di prima:
\begin{align*}
L(v) &= x_1 \cdot L(e_1) + \ldots + x_n \cdot L(e_n) = \\
&= x_1 \cdot B' \times A^1 + \ldots + x_n \cdot B' \times a^n = \\
&= B' \times (x_1 \cdot A^1 + \ldots + x_n \cdot A^n) = \\
&= B' \times A \times X = B' \times X' \tag{$A \times X = X'$}
\end{align*}

\begin{exmp}
$V = \reals_2[x]$, $V' = \reals^4$. Come base di $\reals_2[x]$ prendiamo $B = \{ 1, 1-x, 1-x^2\}$, mentre come base di $V'$ prendiamo la base canonica. Troviamo la matrice $A$ associata ad un'applicazione lineare $L : V \to V'$ e alle basi $B$ e $B_c'$.
\[
A_{4 \times 3} = M_{B_c'}^{B} (L) = (A^1 A^2 A^3)
\]
Dove $A^1$ sono le coordinate di $L(1)$, $A^2$ sono le coordinate di $L(1 - x)$, e $A^3$ sono le coordinate di $L(1 - x^2)$.
\begin{align*}
L(1) &= (0, 1, 0, 0) \\
L(1 - x) &= (0, 1, 0, -1) \\
L(1 - x^2) &= (-1, 0, -1, -1)
\end{align*}
L'immagine di un polinomio generico rispetto alla base $B$ scelta \`e:
\[
L(a_0 + a_1 \cdot x + a_2 \cdot x^2) = (a_2, a_2 + a_0, a_2, a_2 + a_1)
\]
La matrice associata all'applicazione quindi \`e:
\[
A = 
\begin{pmatrix}
0 & 0 & -1 \\
1 & 1 & 0 \\
0 & 0 & -1 \\
0 & -1 & -1
\end{pmatrix}
\]
L'immagine di un generico vettore $p(x)$ \`e:
\[
L \left( p(x) \right) = B' \times A \times X
\]
$B'$ \`e la base di arrivo, $X$ sono le coordinate del vettore rispetto alla base di partenza, e $A$ \`e la matrice che esprime l'applicazione lineare. Bisogna esprimere il vettore $p(x)$ rispetto alla base scelta. Prendiamo il vettore $p(x) = 2 - 2x + x^2 = 1(1) + 2(1-x) - 1(1 - x^2)$. Le sue coordinate sono quindi:
\[
\begin{pmatrix}
1 \\ 2 \\ -1
\end{pmatrix}
\]
E la sua immagine \`e:
\[
L \left( 
\begin{smallpmatrix}
1 \\ 2 \\ -1
\end{smallpmatrix}
\right) =
\begin{pmatrix}
0 & 0 & -1 \\
1 & 1 & 0 \\
0 & 0 & -1 \\
0 & -1 & -1
\end{pmatrix}
\times 
\begin{pmatrix}
1 \\ 2 \\ -1
\end{pmatrix}
=
\begin{pmatrix}
1 \\ 3 \\ 1 \\ -1
\end{pmatrix}
\]
Proviamo a cambiare anche la base di $V'$. $B' = \{ (1,1,1,0), (1,1,0,0), (1,0,0,0), (0,0,0,1)\}$. Dobbiamo trovare la matrice associata a queste due basi, adesso.
\[
\bar A = M_{B'}^{B} (L)
\]
Sempre lo stesso discorso: le colonne della matrice sono le coordinate delle immagini dei vettori della base $B$ rispetto ai vettori della base $B'$. Sappiamo le immagini degli elementi di $B$ rispetto alla base canonica $B_c'$. Le immagini vanno ora espresse rispetto alla nuova base $B'$. Si vede a occhio che le immagini rispetto a $B'$ sono le seguenti:
\begin{align*}
L(1) &= (0,1,0,0) = B' \times 
\begin{smallpmatrix}
0 \\ 1 \\ -1 \\ 0 
\end{smallpmatrix} \\
L(1 - x) &= (0, 1, 0, -1) = B' \times 
\begin{smallpmatrix}
0 \\ 1 \\ -1 \\ -1 
\end{smallpmatrix}\\
L(1 - x^2) &= (-1, 0, -1, -1) = B' \times 
\begin{smallpmatrix}
-1 \\ 1 \\ -1 \\ -1
\end{smallpmatrix}
\end{align*}
Quindi:
\[
\bar A = 
\begin{pmatrix}
0 & 0 & -1 \\
1 & 1 & 1 \\
-1 & -1 & -1 \\
0 & -1 & -1
\end{pmatrix}
\]
Riprendiamo il polinomio $p(x) = 2 - 2x + x^2 = 1(1) + 2(1-x) - 1(1 - x^2)$ e troviamo il suo trasformato. Le sue coordinate per la matrice $\bar A$ mi danno le coordinate del suo trasformato \emph{rispetto alla nuova base di $V'$}, non rispetto alla base canonica di $V'$.
\[
\bar A \times 
\begin{pmatrix}
1 \\ 2 \\ -1 
\end{pmatrix}
= 
\begin{pmatrix}
1 \\ 2 \\ -2 \\ -1
\end{pmatrix}
\]
Quindi l'immagine del nostro $p(x)$ \`e:
\[
L(p(x)) = 1 \cdot (1,1,1,0) + 2 \cdot (1,1,0,0) - 2 \cdot (1,0,0,0) + 1 \cdot (0,0,0,1) =
(1, 3, 1, -1)
\]
Che \`e l'immagine di prima.
\end{exmp}

\subsection{Spazio vettoriale delle applicazioni lineari}

Indichiamo con $\hom(V, V')$ lo spazio vettoriale delle applicazioni lineari da $V$ in $V'$, descriviamo un'applicazione $\varphi_{B'}^{B} : \hom(V, V') \to \matrices_{m \times n} (\field)$ dallo spazio vettoriale delle applicazioni lineari allo spazio vettoriale delle matrici:
\[
{L : V \to V'} \mapsto {\varphi_{B'}^{B} (L) = M_{B'}^{B}}
\]
La dimensione $\dim \hom(V,V')$ \`e $m \times n$, dove le dimensioni degli spazi sono $\dim V = n$ e $\dim V' = m$.

Invece di fare calcoli con le $n$-uple facciamo calcoli con le matrici.

Il nucleo $\ker L$ \`e l'insieme dei vettori $v$ tali che $L(v) = \nullelement$, quindi:
\[
\ker L = \{ v = B \times X : A \times X = 0\}
\]
Dato un vettore $v' \in \image{L}$, per controllare se c'\`e abbiamo un $X'$ tale per cui $v' = B' \times X'$, dobbiamo solo controllare se $X'$ \`e uguale a $A \times X$.

$\varphi$ \`e un isomorfismo. Il suo $\ker \varphi = \{ \nullelement \}$, e l'$\image{\varphi} = \matrices_{m \times n} (\field)$ \`e tutto lo spazio delle matrici.

\subsection{Altre propriet\`a delle applicazioni lineari come matrici}

Data un'applicazione lineare $L : V \to V'$, la matrice associata all'applicazione $L$ rispetto alle basi $B$ e $B'$ (rispettivamente di $V$ e di $V'$), indicata con $M_{B'}^{B} (L)$, \`e la matrice $A$ le cui colonne $A^1, \ldots A^n$ sono le coordinate rispetto a $B'$ di $L(e_1), \ldots L(e_n)$, con $B = \{ e_1, \ldots, e_n \}$.
\[
L(e_i) = B' \times A^i \forall i = 1, \ldots n
\]
L'immagine di un vettore \`e:
\[
L(v) = B' \times (A \times X)
\]
$A \times X$ \`e la colonna delle coordinate di $L(v)$ rispetto a $B'$.

Consideriamo ora le applicazioni lineari $L : V \to V'$ e $ L' : V' \to V''$, e le basi $B$, $B'$ e $B''$ degli spazi $V$, $V'$ e $V''$. Creiamo l'applicazione lineare composta:
\[
L' \circ L : V \to V''
\]
La matrice associata alla composta \`e il prodotto delle matrici associate.
\[
M_{B''}^{B'} (L') \times M_{B'}^{B} (L) = M_{B''}^{B} (L' \circ L)
\]
Sia $L : V \to V'$ un isomorfismo, ossia $L$ \`e iniettiva e $\dim V = \dim V'$, $L$ ha un'inversa $L^{-1} : V' \to V$. Le loro matrici associate sono:
\[
M_{B'}^{B} (L) \times M_{B}^{B'} (L^{-1}) = I =
M_{B}^{B'} (L^{-1}) \times M_{B'}^{B} (L)
\]

\section{Cambiamento di base}

Prendiamo uno spazio vettoriale $V$ di dimensione $\dim V = n$, e due basi di $V$, $B$ e $B'$. Un vettore $v \in V$ si pu\`o scrivere rispetto a entrambi le basi. $v = B \times X$, con $X$ a indicare la colonna delle coordinate di $v$ rispetto a $B$, e $v = B' \times X'$, con $X'$ a indicare la colonna delle coordinate di $v$ rispetto a $B'$.

Come si pu\`o esprimere $X'$ in funzione di $X$? Ossia, ho le coordinate $X$ rispetto a $B$, posso trovare le coordinate $X'$ rispetto a $B'$? Viene molto facile usando le matrici associate alle applicazioni lineari.

Consideriamo l'applicazione identit\`a, $\id : V \to V$, tale per cui $\id(v) = v$. Prendiamo due basi differenti $B$ e $B'$ di $V$. La matrice associata $M_{B'}^{B} (\id)$ avr\`a per colonne le coordinate di $e_1, \ldots e_n \in B$ rispetto a $B'$.
\[
v = \id(v) = B' \times M_{B'}^{B} (\id) \times X
\]
Quindi, sapendo che $v = B \times X = B' \times X'$, segue che $ X' = M_{B'}^{B} (\id) \times X$.

$X' = P \times X$, con $P = M_{B'}^{B} (\id)$, e quindi $P^{-1} = M_{B}^{B'} (\id)$.

\begin{exmp}
Siamo in $\reals^2$. Consideriamo le basi:
\begin{align*}
B &= \{ (1,0), (0,1) \} \\
B' &= \{ (1,1), (1,2) \} \\
v &= (x,y) \in \reals^2 \\
v &= x \cdot e_1 + y \cdot e_2 = B \times 
\begin{pmatrix}
x \\ y
\end{pmatrix} =
B' \times 
\begin{pmatrix}
x' \\ y'
\end{pmatrix}
\end{align*}

Bisogna trovare la matrice associata alla funzione identit\`a $\id : \reals^2 \to \reals^2$ rispetto alle basi $B$ e $B'$.
\[
A = M_{B'}^{B} (\id) = A^1 A^2
\]
Le colonne sono le coordinate dei vettori della base $B$ rispetto alla base $B'$. La prima colonna \`e:
\[
e_1 = (1,0) = a_{1,1} \cdot e_1' + a_{2,1} \cdot e_2' = 2 \cdot (1,1) - 1 \cdot (1,2) \implies A^1 = 
\begin{pmatrix}
2 \\ -1
\end{pmatrix}
\]
Per la seconda colonna, sapendo che $e_2 = B' \times A^2$, vediamo che \`e:
\[
e_2 = (0,1) = a_{1,1} \cdot e_1' + a_{2,1} \cdot e_2' = -1 \cdot (1,1) + 1 \cdot (1,2) \implies A^2 = 
\begin{pmatrix}
-1 \\ 1
\end{pmatrix}
\]
Quindi:
\[
A = 
\begin{pmatrix}
2 & -1 \\
-1 & 1
\end{pmatrix}
\]
Se ora prendo un vettore $w = (3,2)$, per trovare le sue coordinate rispetto alla base $B'$, bisogna moltiplicare la matrice $A$ per la colonna delle coordinate di $w$.
\[
\begin{pmatrix}
2 & -1 \\
-1 & 1
\end{pmatrix}
\times
\begin{pmatrix}
3 \\ 2
\end{pmatrix}
=
\begin{pmatrix}
4 \\ -1
\end{pmatrix}
\]
Infatti $(3,2) = 4 (1,1) - 1 (1,2) = (4,4) - (1,2)$.
\end{exmp}

\begin{exmp}
Un esempio pi\`u complicato.

$\reals_2[x]$ \`e lo spazio vettoriale dei polinomi di grado minore o uguale a 2. $\dim \reals_2[x] = 3$. La base canonica \`e $B_c = \{ 1, x, x^2 \}$. Cambiamo base, e passiamo a $B' = \{ 2 + x + x^2, -x + 2x^2, 2 + x\}$.

Un vettore qualunque $v = p(x) = a + b x + c x^2$, si scrive rispetto alla base canonica come:
\[
p(x) = B_c \times 
\begin{pmatrix}
a \\ b \\ c
\end{pmatrix}
\]
Dobbiamo trovare la matrice associata all'identit\`a rispetto alle due basi. $M_{B'}^{B_c} (\id) = A$. Le sue colonne sono $A^1, A^2, A^3$.
\begin{align*}
e_1 &= B' \times A^1 \\
e_2 &= B' \times A^2 \\
e_3 &= B' \times A^3
\end{align*}
Esprimiamo i vettori della base canonica rispetto alla nuova base:
\begin{align*}
1 &= a' \cdot (2 + x + x^2) + b' \cdot (-x +2x^2) + c' \cdot (2 + x) \to (-1, \sfrac{1}{2}, \sfrac{3}{2}) \\
x &= a' \cdot (2 + x + x^2) + b' \cdot (-x +2x^2) + c' \cdot (2 + x) \to (2, -1, -2) \\
x^2 &= a' \cdot (2 + x + x^2) + b' \cdot (-x +2x^2) + c' \cdot (2 + x) \to (1, 0, -1)
\end{align*}
La matrice quindi \`e:
\[
A = 
\begin{pmatrix}
-1 & 2 & 1 \\
\sfrac{1}{2} & -1 & 0 \\
\sfrac{3}{2} & -2 & -1
\end{pmatrix}
\]
Se ora vogliamo conoscere le coordinate del vettore $v = 2 + x^2$ rispetto alla base nuova:
\[
\begin{pmatrix}
-1 & 2 & 1 \\
\sfrac{1}{2} & -1 & 0 \\
\sfrac{3}{2} & -2 & -1
\end{pmatrix}
\times
\begin{pmatrix}
2 \\ 0 \\ 1
\end{pmatrix} =
\begin{pmatrix}
-1 \\ 1 \\ 2
\end{pmatrix}
\]
Quindi:
\[
2 + x^2 = -1 \cdot (2 + x + x^2) + (-x + 2 x^2) + 2 \cdot (2 + x)
\]
\end{exmp}

\section{Diagonalizzazione}

Sia $L : V \to V$ un endomorfismo. Di solito con gli endomorfismi si lavora con matrici associati a una sola base.
\[
M_{B} (L)
\]
Prendendo un'altra base $B'$, si pu\`o associare un'altra matrice alla stessa applicazione lineare, ma rispetto alla nuova base.
\[
M_{B'} (L)
\]
Le colonne di $A = M_{B} (L)$ sono le coordinate delle immagini dei vettori di $B$ rispetto alla stessa base $B$.
\begin{align*}
B \times A^i &= L(e_i) \\
B' \times {A'}^i &= L(e_i')
\end{align*}
Che relazione c'\`e tra $A$ e $A'$? Sono due matrici associate allo stesso endomorfismo, ma rispetto a due basi diverse.

\begin{figure}[ht]
\centering
\begin{tikzpicture}
  \node (VB) {$V$};
  \node (LVB)   [right of=VB, node distance=4cm] {$V$};
  \node (VB1)   [below of=VB, node distance=4cm] {$V$};
  \node (LVB1)  [right of=VB1, node distance=4cm] {$V$};
  \node (B)     [above of=VB, node distance=1cm] {$B'$};
  \node (LB)    [above of=LVB, node distance=1cm] {$B'$};
  \node (B1)    [below of=VB1, node distance=1cm] {$B$};
  \node (LB1)   [below of=LVB1, node distance=1cm] {$B$};
  \path[->] (VB) edge [above] node {$L$} (LVB)
            (VB) edge [left] node {$\id$} (VB1)
            (VB1) edge [below] node {$L$} (LVB1)
            (LVB1) edge [right] node {$\id$} (LVB)
            ;
\end{tikzpicture}
\caption{$L$ rispetto a $B'$ come composta di due cambi di base con $L$ rispetto a $B$}
\end{figure}
\[
L = id \circ L \circ id
\]
Remember: la matrice associata alla composta \`e la composta delle matrici.
\[
M_{B'} (L) =
M_{B'}^{B} (\id) \times M_{B} (L) \times M_{B}^{B'} (\id)
\]
Abbiamo visto che $M_{B}^{B'} (\id)$ e $M_{B'}^{B} (\id)$ sono l'una l'inversa dell'altra. Chiamiamo $P = M_{B}^{B'} (\id)$. Segue che:
\[
A' = P^{-1} \times A \times P
\]
Questo \`e il legame tra le due matrici associate alla stessa applicazione lineare.

\begin{defn}[Relazione di similitudine]
Due matrici quadrate $A$ e $A'$ si dicono ``simili'' se soddisfano questa condizione, ossia se esiste una matrice invertibile $P$ (quadrata e dello stesso ordine) tale che:
\[
A' = P^{-1} \times A \times P
\]
La relazione $\rho$ per cui $A \rho A' \iff A$ \`e simile a $A'$, \`e una relazione di equivalenza: \`e riflessiva ($A$ \`e simile a s\'e stessa), \`e simmetrica ($A = P \times A' \times P^{-1}$), e banalmente \`e transitiva.

Vale anche il viceversa: se due matrici sono simili, rappresentano lo stesso endomorfismo rispetto a basi diverse.
\end{defn}
\begin{proof}[del viceversa]
$L : V \to V$ \`e un endomorfismo, e $A$ \`e la matrice associata a $L$ rispetto a $B$. Prendiamo poi una matrice $A'$ simile ad $A$. La tesi \`e che $A'$ rappresenta lo stesso morfismo rispetto a un'altra base $B'$.

Essendo simile, $A' = P^{-1} \times A \times P$. Ma $P$ \`e:
\[
P = M_{B}^{B'} (\id)
\]
La nuova base $B'$ \`e proprio $B' = B \times P$.
\[
A' = M_{B'}^{B} (\id) \times A \times M_{B}^{B'} (\id)
\]
$A'$ \`e proprio la matrice $A$ rispetto a una nuova base $B'$.
\end{proof}

% La diagonalizzazione \`e un modo per trovare una matrice diagonale equivalente a una certa matrice data.

Ci sono due problemi equivalenti:
\begin{enumerate}
    \item Data una matrice $A$, esiste una matrice diagonale $D$ simile ad $A$?
    \[
    D = P^{-1} \times A \times P
    \]
    Una matrice diagonale $D$ \`e una matrice $D = (d_{i,j}) $ tale che $ d_{i,j} = 0 $ se $ i \neq j$.
    \item Dato un endomorfismo $L : V \to V$, esiste una base $B$ di $V$ per cui la matrice che rappresenta $L$ rispetto a questa base $B$ \`e una matrice $D$ diagonale?
    \[
    M_{B} (L) = D
    \]
\end{enumerate}

Sia $L : V \to V$ un'applicazione lineare. Se la matrice diagonale $D$ rappresenta $L$ rispetto a $B = \{ e_1, \ldots, e_n \}$, possiamo dire che:
\[
B \times D^{1} = B \times 
\begin{pmatrix}
d_{1,1} \\ 0 \\ 0 \\ \vdots
\end{pmatrix} = L(e_1) = d_{1,1} \cdot e_1
\]
In generale:
\[
L(e_n) = d_{n,n} \cdot e_n
\]
\begin{defn}
Sia $L : V \to V$ un endomorfismo, un vettore $v \in V$ con $v \neq \nullelement$ si dice \emph{autovettore} di $L$ se $\exists \lambda \in \field$ tale che $L(v) = \lambda \cdot v$, ossia l'immagine di $v$ \`e un suo multiplo.
\end{defn}

Se $D$ \`e una matrice diagonale che rappresenta $L$ rispetto alla base $B$, $B$ \`e una base di autovettori. Viceversa, se esiste una base $B$ di $V$ formata da autovettori di $L$, allora la matrice associata a $L$ rispetto a $B$ \`e diagonale.

Sia $v$ un autovettore di $L$, ossia tale per cui $L(v) = \lambda \cdot v$, $\lambda$ si dice \emph{autovalore} di $L$.

Un autovalore di $L$ \`e uno scalare tale per cui esiste un vettore $v$ la cui immagine \`e l'autovalore moltiplicato per il vettore $v$.

$\lambda$ pu\`o essere 0: quando $\lambda$ \`e 0, il vettore $v$ va nel vettore nullo. Quindi $L$ non \`e un'applicazione iniettiva.

$\lambda = 0$ \`e un autovalore di $L \iff \ker L \neq \{ \nullelement \} \iff L$ non \`e iniettiva.

Se $L$ non \`e iniettiva, allora tutti i vettori $v \neq \nullelement$ di $\ker L$ sono autovettori di autovalore 0.

Come si trovano gli autovalori e gli autovettori di un endomorfismo (e quindi della matrice associata all'endomorfismo)?

Consideriamo $L : V \to V$ e la base $B$. Abbiamo la matrice $A$ associata a $L$ rispetto a questa base, ossia $A = M_{B} (L)$.
\begin{align*}
L(v) = \lambda \cdot v \implies \\
L(v) = \lambda \cdot \id(v) \implies \\
(L - \lambda \cdot \id) (v) = \nullelement \implies \\
(A - \lambda \cdot I) = M_B (L - \lambda \cdot id)
\end{align*}
Quindi, se $v = B \times X$, ossia $X$ sono le coordinate di $v$ rispetto alla base $B$, si ha che:
\[
(A - \lambda \cdot I) \times X = \nullelement
\]
Se $v$ esiste, allora $X$ \`e soluzione non nulla di questo sistema. Se il $\det{A - \lambda \cdot I} \neq 0$, abbiamo che $X = \nullelement$, quindi deve essere il determinante $\det{A - \lambda \cdot I} = 0$.

Gli autovalori di $L$ sono gli zeri del polinomio $\det{A - \lambda \cdot I}$, detto anche polinomio caratteristico.

Una volta trovati gli autovalori, come si trovano gli autovettori? Risolvendo il sistema $(A - \lambda \cdot I) \times X = \nullelement$.

Prendendo un'altra matrice $A'$ associata a $L$ rispetto a un'altra base, questa ha lo stesso polinomio caratteristico.
\[
\det{A - \lambda \cdot I} = \det{A' - \lambda \cdot I}
\]
Il polinomio caratteristico di $L$ \`e un invariante di $L$, ossia se $A'$ \`e un'altra matrice associata a $L$, si ha che $\det{A - \lambda \cdot I} = \det{A' - \lambda \cdot I}$.

\begin{proof}
$A'$ rappresenta lo stesso endomorfismo $L$ rappresentato da $A$. $A' = P^{-1} \times A \times P$.
\begin{align*}
\det{A' - \lambda \cdot I} &= \det{ P^{-1} \times A \times P - \lambda \cdot I} = \\
&= \det{P^{-1} \times A \times P - \lambda \cdot P^{-1} \times P} = \tag{si pu\`o mettere in evidenza} \\
&= \det{P^{-1} \times (A - \lambda \cdot I) \times P} = \tag{per Binet} \\
&= \det{P^{-1}} \cdot \det{A - \lambda \cdot I} \cdot \det{P} = \\
&= \frac{1}{\det{P}} \cdot \det{A - \lambda \cdot I} \cdot \det{P} =
\det{A - \lambda \cdot I}
\end{align*}
\end{proof}

Data una matrice $A$, $\lambda$ \`e autovalore di $A$ se $A \times X = \lambda \cdot X$.

Il polinomio caratteristico di $A$ \`e $\det{A - \lambda \cdot I}$. Matrici simili $A$ e $A'$, ossia $A \rho A'$, hanno gli stessi autovalori. \emph{Non} hanno gli stessi autovettori: le coordinate cambiano.

L'applicazione $L : V \to V$ \`e diagonalizzabile, cio\`e esiste una matrice diagonale $D$ che la rappresenta, se e solo se $V$ ha una base di autovettori di $L$.

Autovettori relativi ad autovalori distinti sono indipendenti. Condizione sufficiente affinch\'e $L : V \to V$ sia diagonalizzabile, \`e che $L$ abbia $\dim V = n$ autovalori distinti, ossia $n$ autovettori indipendenti.

$\det{A - \lambda \cdot I}$ \`e un polinomio di grado $n$, che quindi ammette al pi\`u $n$ radici.
\[
(A - \lambda \cdot I) =
\begin{pmatrix}
a_{1,1} - \lambda & \dots & a_{n,1} \\
\vdots & \ddots & \vdots \\
a_{1,n} & \dots & a_{n,n} - \lambda
\end{pmatrix}
\]
Quella sopra non \`e condizione necessaria: si possono avere meno autovalori distinti, ma avere comunque una base.

Dato un autovalore $\lambda$, con $E(\lambda)$ si indica l'insieme degli autovettori di autovalore $\lambda$, pi\`u il vettore nullo.
\[
E(\lambda) = \{ v \in V : L(v) = \lambda \cdot v \}
\]
$E(\lambda)$ \`e un sottospazio di $V$, e si dice autospazio relativo all'autovalore $\lambda$.

Ciascun autovalore $\lambda_1, \ldots \lambda_t$ avr\`a un suo autospazio $E(\lambda_i)$, e ciascun autospazio avr\`a una base $B_i$, con dimensione $\dim E(\lambda_i) = n_i$. Se la somma delle dimensioni fa $n$, ossia:
\[
\dim E(\lambda_1) + \ldots + \dim E(\lambda_t) = n
\]
allora $L$ \`e diagonalizzabile.

Abbiamo visto che, data un'applicazione lineare $L : V \to V$, con $\dim V = n$, e due basi di $V$, $B = \{ e_1, \ldots, e_n \}$ e $B' = \{ e_1', \ldots e_{n}' \}$, abbiamo due matrici associate a $L$ rispetto alle due basi, ossia $M_{B}(L)$ e $M_{B'}(L)$.

Sapendo che $L = \id \circ L \circ \id$, e che, data un'applicazione composta $G = L \circ F$, la matrice associata a $G$ \`e il prodotto delle matrici associate a $L$ e a $F$:
\[
M_{B''}^{B} (G) = M_{B''}^{B'} (L) \times M_{B'}^{B} (F)
\]
Sapendo questo, dicevamo, si vede che:
\[
M_{B'} (L) = M_{B'}^{B} ( \id ) \times M_{B} (L) \times M_{B}^{B'} (\id)
\]
Due matrici $A$, $A'$ si dicono simili se esiste una matrice invertibili $P$ per cui:
\[
A' = P^{-1} \times A \times P
\]
\begin{prop}
$A$ e $A' \in \matrices_{n} (\field)$ sono simili $\iff$ rappresentano lo stesso endomorfismo rispetto a due basi diverse.
\end{prop}
Da questa proposizione sorgono due problemi equivalenti:
\begin{enumerate}
    \item Determinare, qualora esista, una matrice diagonale $D$ simile a una matrice data $A$.
    \item Determinare, qualora esista, una matrice $D$ diagonale che rappresenta un endomorfismo $L$.
\end{enumerate}

Dato un endomorfismo $L$ (o una matrice $A$) si dice \emph{autovettore} di $L$ (o di $A$) un vettore $v \neq \nullelement$ (nel caso della matrice, una $n$-upla diversa dalla $n$-upla nulla) tale che $\exists \lambda \in \reals$ per cui $L(v) = \lambda \cdot v$. Nel caso della matrice, $A \times X = \lambda \cdot X$.
\[
A =
\begin{pmatrix}
3 & 0 & 0 \\ 
0 & 1 & 0 \\
0 & 0 & -1
\end{pmatrix}
=
M_B(L)
\]
\[
B = \{ v_1, v_2, v_3 \}
\]
\begin{align*}
L(v_1) &= 3 \cdot v_1 + 0 \cdot v_2 + 0 \cdot v_3 = 3 \cdot v_1 \\
L(v_2) &= v_2 \\
L(v_3) &= - v_3
\end{align*}
Una matrice diagonale rappresenta $L$ rispetto ad una base $B$ se la base $B$ \`e una base di autovettori.

\begin{prop}
Condizione necessaria e sufficiente affinch\'e $L : V \to V$ sia diagonalizzabile \`e che $V$ abbia una base di autovettori.

Con le matrici, condizione necessaria e sufficiente affinch\'e una matrice $A \in \matrices_{n} (\reals)$ sia diagonalizzabile \`e che in $\reals^n$ ci sia una base composta da autovettori di $A$.
\end{prop}
La matrice diagonale $D$ ha sulla diagonale gli autovalori di $L$ (rispettivamente di $A$).

\begin{prop}
Gli autovalori di $L$ (rispettivamente della matrice $A$) sono gli zeri del polinomio caratteristico di $L$ (rispettivamente di $A$), ossia del polinomio $\det{A - \lambda \cdot I}$.
\end{prop}

Come si trovano gli autovettori, invece?
\[
L(V) = \lambda \cdot v \implies L(V) - \lambda \cdot v = \nullelement \implies 
(A \times X - \lambda \cdot X) = \nullelement \implies (A - \lambda \cdot I) \times X = 0
\]
Si risolve il sistema e si trovano gli autovettori.

\subsection{Criterio di diagonalizzazione}

Dobbiamo introdurre altre due proposizioni.

$\lambda$ \`e un autovalore (con $\lambda \in \reals$) $\iff \det{A - \lambda \cdot I} = 0$.

Quindi $\lambda$ ha una molteplicit\`a come radice del polinomio caratteristico, detta molteplicit\`a algebrica di $\lambda$. Si indica con $m_a(\lambda)$.

Prendiamo tutti gli autovettori di autovalore $\lambda$.
\[
E(\lambda) = \{ v \in V : L(v) = \lambda \cdot v \}
\]
$E(\lambda)$ contiene gli autovettori di $L$ di autovalore $\lambda$, pi\`u il vettore nullo. $E(\lambda)$ \`e un sottospazio di $V$. Infatti, presi due elementi, contiene tutte le loro combinazioni lineari.
\begin{align*}
v, w &\in E(\lambda) \\
a \cdot v + b \cdot w &\in E(\lambda)
\end{align*}
Infatti $L(a \cdot v + b \cdot w) = a \cdot L(v) + b \cdot L(w) = 
a \cdot \lambda \cdot v + b \cdot \lambda \cdot w =
\lambda \cdot (a \cdot v + b \cdot w)$.

Anche questo sottospazio, $E(\lambda)$, ha una sua dimensione, viene chiamata molteplicit\`a geometrica, e viene solitamente indicata con $m_g(\lambda)$.

La molteplicit\`a algebrica di $\lambda$ \`e sempre minore o uguale di $n = \dim V$. Infatti $\det{A - \lambda \cdot I}$ ha grado $n$.

La molteplicit\`a geometrica, invece, che \`e la dimensione $\dim E(\lambda)$, chiamato \emph{autospazio relativo a $\lambda$}, \`e sempre minore o uguale della molteplicit\`a algebrica. Quindi:
\[
m_g(\lambda) \le m_a(\lambda) \le n
\]
Infatti $\dim E(\lambda) = m_g(\lambda) = t$. La dimensione \`e la dimensione di una base di $E(\lambda)$, contenente $t$ vettori. La base $B = \{v_1, \ldots v_t \}$ contiene tutti vettori del tipo $L(v_i) = \lambda \cdot v_i$.

Ma $E(\lambda) \subseteq V$, ed essendo che $\dim V = n$, deve essere che $t \le n$. Deve esserci una base $\bar{B}$ di $V$ contenente la base $B$ di $E(\lambda)$.
\[
\bar{B} = \{ v_1, \ldots, v_t, w_{t+1}, \ldots, w_n \}
\]
La molteplicit\`a algebrica \`e almeno $t$.
% $\det{A - \lambda \cdot I} = 0$, ha $A$ nella forma:
% \[
% M_{\bar{B}} (L) = A =
% \begin{pmatrix}
% \lambda & 0 & \dots & 0 & \dots \\
% 0 & \lambda & 0 & 0 & \dots\\
% \vdots & \vdots & \ddots & \vdots
% \dots & \dots & 0 & \lambda
% ...
% \end{pmatrix}
% % fare matrice con \lambda solo all'inizio
% \]
\begin{prop}
Se $v_1, \ldots, v_t$ sono autovettori a due a due distinti relativi rispettivamente agli autovalori $\lambda_1, \ldots, \lambda_t$ (con $\lambda_i \neq \lambda_j$ se $i \neq j$), costituiscono un insieme indipendente.
\end{prop}

\begin{proof}
Per induzione sul numero $t$ degli autovettori. Per $t = 1$ \`e vero, abbiamo un solo autovettore diverso dal vettore nullo (per definizione), che quindi costituisce un insieme indipendente.

Supponiamo il teorema vero per $t \ge 1$, e dimostriamo che \`e vero per $t + 1$ vettori. Se fossero dipendenti, il vettore $v_{t+1}$ apparterrebbe allo spazio generato $\pow{v_1, \ldots, v_t}$.
\[
\nullelement = a_1 \cdot v_1 + \ldots + a_t \cdot v_t + a_{t+1} \cdot v_{t+1}
\]
\begin{align*}
\nullelement = L( \nullelement ) &= a_1 \cdot L(v_1) + \ldots + a_t \cdot L(v_t) + a_{t+1} \cdot L(v_{t+1}) = \tag{essendo tutti autovettori} \\
&= a_1 \cdot \lambda_1 \cdot v_1 + \ldots + a_t \cdot \lambda_t \cdot v_t + a_{t+1} \cdot \lambda_{t+1} \cdot v_{t+1}
\end{align*}
Ogni $\lambda_i, \lambda_j$ \`e una coppia di elementi a due a due distinti. Quindi al pi\`u un $\lambda_i$ \`e pari a 0.

Sia $v_j$ un vettore tale che $\lambda_j \neq 0$. Supponiamo $v_j$ sia proprio $v_{t+1}$, quindi $\lambda_{t+1} \neq 0$.

Moltplicando per $\lambda_{t+1}$, si ottiene che:
\[
\nullelement = \lambda_{t+1} \cdot a_1 \cdot v_1 + \ldots + \lambda_{t+1} \cdot a_{t+1} \cdot v_{t+1}
\]
E inoltre ho sempre l'identit\`a per cui:
\[
\nullelement = \lambda_{1} \cdot a_{1} \cdot v_{1} + \ldots + \lambda{t+1} \cdot a_{t+1} \cdot v_{t+1}
\]
Sottraendo le due equazioni si ottiene che:
\[
\nullelement = a_1 \cdot (\lambda_{t+1} - \lambda_{1}) \cdot v_1 + \ldots + a_t \cdot (\lambda_{t+1} - \lambda{t}) \cdot v_t
\]
Essendo $v_1, \ldots, v_t$ vettori indipendenti, e sapendo che ogni coppia $\lambda_i, \lambda_j$ \`e formata da elementi distinti, e che:
\[
a_1 \cdot (\lambda_{t+1} - \lambda_1) = \ldots = 
a_t \cdot (\lambda_{t+1} - \lambda_t) = 0
\]
Ogni $a_1, \ldots, a_t$ deve essere a 0. $\lambda_{t+1}$ si era detto essere diverso da zero, quindi anche $a_{t+1}$ deve essere zero.
\end{proof}

\begin{prop}
Le seguenti proposizioni sono equivalenti:
\begin{enumerate}
    \item\label{itm:diag_1} L'applicazione lineare $L$ \`e diagonalizzabile (o la matrice $A$ \`e diagonalizzabile)
    \item\label{itm:diag_2} Esiste una base di $V$ formata da autovettori dell'applicazione lineare $L$ (rispettivamente della matrice $A$)
    \item\label{itm:diag_3} $V$ \`e somma diretta di autospazi, ossia $V = E(\lambda_1) \oplus \ldots \oplus E(\lambda_t)$, ossia ogni vettore $v \in V$ si esprime in un solo modo come somma di autovettori
    \item\label{itm:diag_4} Ogni autovalore di $L$ ha molteplicit\`a geometrica uguale a quella algebrica, e la somma delle molteplicit\`a \`e:
    \[
    \sum_{\lambda} m_a(\lambda) = \sum_{\lambda} m_g(\lambda) = n
    \]
\end{enumerate}
\end{prop}
\begin{proof}
\ref{itm:diag_1} implica \ref{itm:diag_2} e \ref{itm:diag_2} implica \ref{itm:diag_1} lo abbiamo gi\`a visto.

\ref{itm:diag_3} implica che una base $B$ di $V$ si ottiene dall'intersezione $B_1 \cup \ldots \cup B_t$ dove $B_i$ \`e una base di $E(\lambda_i)$. Autovettori relativi ad autovalori distinti sono indipendenti.

Le molteplicit\`a geometriche sono le dimensioni delle basi, quindi viene subito anche la \ref{itm:diag_4}.

\ref{itm:diag_2} implica la \ref{itm:diag_3}, perch\'e data una base di autovettori posso dividerli in base al loro autovalore.

% riscrivere bene queste dimostrazioni
\end{proof}

\begin{exmp}
Consideriamo l'applicazione lineare $T : \reals^3 \to \reals^3$. Si pu\`o definire un'applicazione lineare in tre modi:
\begin{enumerate}
    \item dire quanto vale $T(x,y,z)$;
    \item dire qual \`e la matrice $A$ associata a $T$ rispetto a una base $B$:
    \[
    T(x,y,z) = A \times 
    \begin{pmatrix}
    x' \\ y' \\ z'
    \end{pmatrix}
    \]
    dove $(x', y', z')$ sono le coordinate di $(x,y,z)$ rispetto a $B$;
    \item dire quali valori assume $T$ sui valori di una base $B$, ad esempio $T(1,1,1)$, $T(1,1,0)$, $T(1,0,0)$.
\end{enumerate}
Diciamo i valori che $T$ assume sulla base:
\begin{align*}
T(1,1,1) &= (3,0,6) \\
T(1,1,0) &= (3,1,2) \\
T(1,0,0) &= (2,0,0)
\end{align*}
Si vede subito che $(1,0,0)$ \`e un autovettore di autovalore 2, infatti $T(1,0,0) = 2 \cdot (1,0,0)$.

Mettendo i valori per colonna si ha una matrice associata a $T$ rispetto alla base $B$ del dominio, e alla base canonica del codominio:
\[
M_{B_c}^{B} (L) =
A = 
\begin{pmatrix}
3 & 3 & 2 \\
0 & 1 & 0 \\
6 & 2 & 0
\end{pmatrix}
\]
Bisogna trovare la matrice associata alla base $B$, ossia $M_{B} (L)$. La prima colonna di questa nuova matrice sar\`a la prima colonna della matrice $A$ sopra, espressa rispetto ai vettori della base $B$.
\[
(3,0,6) = a \cdot (1,1,1) + b \cdot (1,1,0) + c \cdot (1,0,0)
\]
Se si vuole invece trovare la matrice $M_{B_c} (L)$, rispetto alla base canonica:
\begin{align*}
T(1,0,0) &= (2,0,0) \\
T(0,1,0) &= T((1,1,0) - (1,0,0)) = T(1,1,0) - T(1,0,0) = (1,1,2) \\
T(0,0,1) &= (0,-1,4)
\end{align*}
Quindi:
\[
M_{B_c} (L) = 
\begin{pmatrix}
2 & 1 & 0 \\
0 & 1 & -1 \\
0 & 2 & 4
\end{pmatrix}
\]
Determiniamo gli autovalori di $L$.
\begin{align*}
\det{A - \lambda \cdot I} &= 
\det{
\begin{pmatrix}
(2- \lambda) & 1 & 0 \\
0 & (1 - \lambda) & -1 \\
0 & 2 & (4 - \lambda)
\end{pmatrix}
} = \\
&= (2 - \lambda) \cdot
\left[
\det{
\begin{pmatrix}
(1 - \lambda) & -1 \\
2 & (4 - \lambda)
\end{pmatrix}
}
\right] = \\
&= (2 - \lambda) \cdot \left[
(1 - \lambda) \cdot (4 - \lambda) + 2
\right] =
(\lambda - 2)^2 \cdot (\lambda - 3)
\end{align*}
Quindi ha due autovalori: $\lambda_1 = 2$ e $\lambda_2 = 3$. Le molteplicit\`a algebriche sono $m_a(2) = 2$, e $m_a(3) = 1$.

Troviamo gli autospazi.
\[
E(3) = \{ v \in \reals^3 : T(v) = 3 \cdot v \}
\]
Tutti i $v \in E(3)$ hanno coordinate $X$ rispetto alla base canonica che risolvono il sistema $(A - 3 \cdot I) \times X = 0$.
\[
\begin{pmatrix}
-1 & 1 & 0 \\
0 & -2 & -1 \\
0 & 2 & 1
\end{pmatrix}
\times
\begin{pmatrix}
x \\ y \\ z 
\end{pmatrix} = 0 
\]
\`E equivalente al sistema:
\[
\begin{cases}
-x + y = 0 \\
2y + z = 0
\end{cases}
\]
Le soluzioni sono $z = - 2 \, y$ e $x = y$, ossia $(x, x, -2 \, x)$.
\[
E(3) = \{ (x,x,-2 \, x) \in \reals^3 : x \in \reals \}
\]
La sua dimensione \`e $\dim E(3) = 1$, quindi $E(3)$ \`e isomorfo a $\reals$, e l'isomorfismo \`e quello che associa a un certo $y \mapsto (y, y, -2 \, y)$.
\[
E(3) = \pow{(1,1,-2)}
\]
Troviamo gli autovettori di autovalore 2:
\[
E(2) = \{ v : T(v) = 2 \cdot v \}
\]
Il sistema da risolvere \`e:
\[
\begin{pmatrix}
0 & 1 & 0 \\
0 & -1 & -1 \\
0 & 2 & 2
\end{pmatrix}
\times
\begin{pmatrix}
x \\ y \\ z
\end{pmatrix} = 0
\]
Anche in questo caso la dimensione $\dim E(2) = 1$, quindi $T$ non \`e diagonalizzabile. Infatti:
\[
m_g(2) = 1 < m_a(2) = 2
\]
Sapevamo gi\`a da prima che $E(2) = \pow{(1,0,0)}$.
\end{exmp}
