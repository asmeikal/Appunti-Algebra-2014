
\begin{center}
\indent
\textit{Gruppi, anelli, campi. In particolare, anello degli interi modulo $m$ intero, anello dei polinomi.}
\end{center}

\section{Strutture algebriche con un'operazione}

Una struttura algebrica \`e una coppia $(A, \cdot)$ dove A \`e un'insieme e $\cdot$ \`e un'operazione $\cdot : A \times A \to A$. Ad esempio $(\mathbb{N}, +)$ \`e una struttura algebrica.

Le operazioni sono funzioni definite su prodotti cartesiani a valori in un insieme. Un'operazione binaria \`e definita sul prodotto cartesiano fra due insiemi.

Riprendendo la composizione, dati tre insiemi $A, B, C$, $B^A$ \`e l'insieme delle funzioni da $A$ in $B$, $C^B$ \`e l'insieme delle funzioni da $B$ in $C$. La composizione $\circ$ \`e un'operazione definita sul prodotto cartesiano degli insiemi $B^A \times C^B \to C^A$.

Posso rappresentare un'operazione come funzione $(\circ \left( f, g \right))$ o inserendo l'operatore fra i due operandi $ (g \circ f) $.
\begin{gather*}
f: \mathbb{R} \times \mathbb{R} \to \mathbb{R} ; \
f(x,y) = \sqrt{2} \cdot x + y \\
g: \mathbb{R} \to \mathbb{R} \times \mathbb{R} ; \
g(z) = (0,z) \\
g \circ f : \mathbb{R} \times \mathbb{R} \to \mathbb{R} \times \mathbb{R} \\
(x,y) \xrightarrow{f} \sqrt{2} \cdot x + y = z \xrightarrow{g} \left( 0, \sqrt{2}x + y \right) \\
f \circ g : \mathbb{R} \to \mathbb{R} \\
z \xrightarrow{g} (0,z) \xrightarrow{f} \sqrt{2} \cdot 0 + z = z
\end{gather*}

Una struttura algebrica \`e un'insieme su cui \`e definita un'operazione che prende due elementi di quell'insieme e gliene associa un terzo.

Le strutture vengono classificate in base alle loro propriet\`a:
\begin{description}
    \item[Propriet\`a associativa\label{itm:strutture_associativa}] $\forall \ a, b, c \in A : a \cdot (b \cdot c) = (a \cdot b) \cdot c$
    \item[Elemento neutro\label{itm:strutture_neutro}] Esistenza di un'elemento neutro, o elemento identit\`a. $1 \in A : \forall \ a \in A , a \cdot 1 = a = 1 \cdot a$
    \item[Propriet\`a commutativa\label{itm:strutture_commutativa}] $ \forall a, b \in A , a \cdot b = b \cdot a $. In una struttura algebrica commutativa in genere l'identit\`a si indica con 0.
    \item[Inverso\label{itm:strutture_inverso}] Esistenza dell'inverso. $ \forall a \in A \ \exists \ b \in A $ t.c. $a \cdot b = 1 = b \cdot a $.
\end{description}

\subsection{Classificazione delle strutture algebriche con una operazione}

Per essere studiabile, una struttura algebrica deve essere quantomeno associativa.

\begin{description}
    \item[Semigruppo] struttura algebrica associativa.
    \item[Monoide] struttura algebrica associativa con elemento identit\`a.
    \item[Gruppo] struttura algebrica associativa con elemento identit\`a e con inverso (ossia, monoide con inverso).
    \item[Gruppo abeliano] struttura algebrica che presenta tutte e quattro le propriet\`a: associativa, elemento neutro, commutativa, inverso.
\end{description}

La struttura algebrica $\left( \mathbb{N}, + \right)$ \`e un monoide commutativo. Anche $\left( \mathbb{N}, \cdot \right)$ \`e un monoide commutativo. $\left( \mathbb{Z}, + \right)$ \`e un gruppo perch\`e esiste l'inverso. $\left( \mathbb{Z}, \cdot \right)$ \`e un monoide, perch\'e non ha l'inverso.

\subsection{Gruppo simmetrico}

\begin{defn}[Gruppo simmetrico]
Il prototipo di tutti i gruppi \`e il gruppo simmetrico su $n$ elementi, il cui insieme \`e indicato con $S_n$. Prendiamo un insieme $E = \left\{ e_1, \dots, e_n \right\}$.
\[
S_n = \left\{ f : E \to E \text{ t.c. $f$ \`e biunivoca} \right\}
\]
Quindi $S_n$ \`e l'insieme di tutte le permutazioni degli elementi di $E$. Il gruppo simmetrico \`e definito sull'insieme $S_n$ e l'operazione \`e la composizione: $\left( S_n, \circ \right)$.
\begin{enumerate}
    \item $f \circ \left( g \circ h \right) = \left( f \circ g \right) \circ h$ 
    \item L'unit\`a \`e la funzione identica (o identit\`a) $i_E : E \to E$ tale che $\forall \ e \in E $ $i_E(e) = e$. $f \circ i_E = f = i_E \circ f$ 
    \item Una funzione biunivoca $f$ ha una funzione inversa $g$. $g : E \to E $ t.c. $ g(f(e)) = e$.
\end{enumerate}
\end{defn}

Una funzione $f : E \to E $ iniettiva su un insieme finito $E$ \`e necessariamente suriettiva e quindi biunivoca. Un insieme \`e finito se non pu\`o essere messo in corrispondenza biunivoca con un suo sottoinsieme proprio.

% \subsection{Punto di vista dell'occupazione}

% $f : \left \{ 1, \dots, 6 \right \} \to \left \{ 1, \dots, 6 \right \}$. Penso il dominio come degli oggetti. Il codominio come dei ``cassetti''. La funzione \`e un modo di mettere gli oggetti del dominio nei ``cassetti''.

% \begin{tabular}{cccccc}
% 1 & 2 & 3 & 4 & 5 & 6 \\
% 2 & 3 & 5  &1 & 6 & 4
% \end{tabular}
% \`E un'occupazione.

% \begin{tabular}{cccccc}
% 1 & 2 & 3 & 4 & 5 & 6 \\
% 6 & 6 & 3 & 5 & 5 & 5
% \end{tabular}
% Non \`e un'occupazione.


\section{Monoidi}

Un monoide $(M, \cdot)$ \`e una struttura algebrica con un'operazione $\cdot : M \times M \to M$ tale che:
\begin{description}
    \item[1M] L'operazione $\cdot$ \`e associativa;
    \item[2M] $\exists \ 1_M $ t.c. $ \forall \ a \in M$ $ 1_M \cdot a = a = a \cdot 1_M$, ossia esiste l'elemento identit\`a. 
\end{description}

Un sottomonoide $(S, \cdot)$ con $S \subseteq M$ \`e un monoide in cui esiste l'operazione $\cdot : S \times S \to S$, ossia $S$ \`e chiuso, cio\`e $s \cdot s' \in S$ e $1_M \in S$.

Ad esempio, considerando $(\mathbb{N}, +)$, $(P, +)$ con $P = \{ m \in \mathbb{N} : \exists \ k $ t.c. $m = 2k \}$ \`e un sottomonoide di $(\mathbb{N},+)$ perch\`e la somma di due pari \`e pari e lo 0 appartiene ai pari.

$k \mathbb{N} = \{ m \in \mathbb{N} : \exists \ t \in \mathbb{N} $ t.c. $ m = k t\}$ \`e la ``versione generale'' dell'insieme dei numeri pari.

Considerando $(\mathbb{N}, \cdot)$ e l'elemento neutro 1, i pari non sono un sottomonoide perch\'e non hanno l'elemento neutro, ma i dispari s\`i.

\subsection{Morfismi di monoidi}

\begin{defn}
Dati i monoidi $(M, \cdot)$ e $(A, \ast)$, un morfismo di monoidi \`e un'applicazione $f : M \to A$ che conserva le strutture, ossia tale che $\forall \ x,y \in M $ ho che $f(x \cdot y) = f(x) \ast f(y)$. Inoltre, $f(1_M) = 1_A$.
\end{defn}

\subsection{Teorema di omomorfismo per i monoidi\label{omomorfismo_monoidi}}

\begin{prop}
Sia $f : (M, \cdot) \to (A, \ast)$ un morfismo di monoidi, $f$ definisce una relazione di equivalenza $\varepsilon_f$ tale che $\ker f = M / \varepsilon_f \cong Im_f$, ossia il quoziente \`e isomorfo all'immagine. Inoltre il quoziente ha una struttura di monoide:
\[
(M / \varepsilon_f , \cdot) \cong (Im_f, \ast)
\]
con $(Im_f, \ast)$ sottomonoide di $(A, \ast)$.
\end{prop}
\begin{proof}
Ogni morfismo di monoidi $f : (M, \cdot) \to (A, \ast)$ individua un sottomonoide di $(A, \ast)$, che \`e $(Im_f, \ast)$. 

Essendo $f$ un morfismo di monoidi, $\forall f(x), f(y) \in Im_f$, $f(x) \ast f(y) = f(x \cdot y) \in Im_f$, quindi $Im_f $ \`e chiuso rispetto a $\ast$. Devo poi verificare che $1_A \in Im_f \Leftarrow f(1_M) = 1_A$.

Anche $(\ker f, \cdot)$ \`e un monoide. Dobbiamo dimostrare l'esistenza dell'isomorfismo con $Im_f$.

$\ker f$ \`e l'insieme delle classi di equivalenza $[x] = \{ y \in M : x \ \varepsilon_f \ y \Leftrightarrow f(x) = f(y) \} \in \ker f$.

Definiamo l'operazione di prodotto fra classi come la classe del prodotto di due rappresentanti $[x] \cdot [z] = [x \cdot z]$ qualsiasi rappresentante scelgo della classe. Bisogna verificare che questa definizione sia indipendente dai rappresentnati! Lo faremo nella sezione \ref{congruenze}, in particolare nella dimostrazione \ref{congruenza_monoidi}. 

L'operazione fra classi \`e associativa, perch\'e \`e associativa l'operazione fra rappresentanti. Inoltre ho l'unit\`a $[1_M]$ in $\ker f$.

L'isomorfismo fra $(Im_f, \ast)$ e $(\ker f, \cdot)$ segue naturalmente dal fatto che $\ker f $ e $Im_f$ sono in biezione. Inoltre, essendo entrambi dei monoidi, la funzione $f$ \`e un isomorfismo di monoidi.
\end{proof}

\subsection{Potenze (iterazioni sui monoidi)}

\begin{defn}[Potenze]
A partire dal monoide $(M, \cdot)$ possiamo definire le iterazioni dell'operazione $\cdot$, ossia le potenze.

Sia $ a \in M$, si definisce:
\begin{enumerate}
    \item $a^0 = 1_M$
    \item $a^{n+1} = a \cdot a^{n}$
\end{enumerate}
\end{defn}
\begin{prop}[Commutativit\`a della potenza]
$\forall \ n \in \mathbb{N}$, $a \cdot a^n = a^n \cdot a$, ossia la potenza \`e commutativa.
\end{prop}
\begin{proof}
Si dimostra per induzione. Si vede subito che con $n = 0$, per definizione $a \cdot a^0 = a = a^0 \cdot a$.

Per definizione di potenza $a \cdot a^{n+1} = a \cdot a \cdot a^{n} $, per ipotesi induttiva $ a \cdot a^n \cdot a $ che di nuovo per definizione di potenza \`e $ a^{n+1} \cdot a$.
\end{proof}
\begin{prop}
Valgono tutte le propriet\`a tipiche delle potenze:
\[
a^{m + n} = a^m \cdot a^n
\]
\end{prop}
\begin{proof}
Si dimostra anche questo per induzione su $n$. Con $n = 0$, $a^{m+0} = a^m = a^m \cdot 1_M = a^m \cdot a^0$.

Passo induttivo. $a^{m + n + 1} = a \cdot a^{m + n}$ per definizione di potenze. Applicando l'ipotesi induttiva, $a \cdot a^{m + n} = a \cdot a^m \cdot a^n$. Per commutativit\`a $a \cdot a^m \cdot a^n = a^m \cdot a \cdot a^n = a^m \cdot a^{n+1}$.
\end{proof}

\begin{theorem}
Dato un monoide $(M, \cdot)$ ed un elemento $a \in M$, esiste un solo morfismo di monoidi $f : (\mathbb{N}, +) \to (M, \cdot)$ tale che $f(1) = a$, ed \`e $f(n) = a^n$.
\end{theorem}
\begin{proof}
$f$ \`e un morfismo di monoidi, quindi deve verificare che $f(m+n) = f(m) \cdot f(n)$ e che $f(0) = 1_M$. 

Per le propriet\`a delle potenze dimostrate precedentemente, $f(m+ n) = a^{m+n} = a^m \cdot a^n = f(m) \cdot f(n)$, e $f(0) = a^0 = 1_M$ per la definizione delle potenze. 

Inoltre verifica la condizione $f(1) = a$, infatti $f(1) = a^1 = a \cdot a^0 = a \cdot 1_M = a$.

Dobbiamo dimostrare l'unicit\`a di $f$. Sia $g : (\mathbb{N}, +) \to (M, \cdot )$ un morfismo tale che $g(1) = a$, dimostriamo che $\forall n \in \mathbb{N} $ $ g(n) = f(n) = a^n$.

Dimostriamolo per induzione su $n$. Per definizione di morfismo di monoidi, $g(0) = 1_M = f(0) = a^0$.

Supponiamo che $g(n) = f(n)$, per definizione di morfismo di monoidi $g(n+1) = g(1) \cdot g(n) = a \cdot g(n) = a \cdot f(n) = a \cdot a^n = a^{n+1}$.
\end{proof}

\begin{exmp}
Sia $\Gamma$ un insieme, la struttura algebrica $(\mathbb{P}(\Gamma), \cup)$ \`e in particolare un monoide. L'unione \`e associativa ($(A \cup B) \cup C = A \cup (B \cup C)$), ed esiste l'elemento neutro $\emptyset$.

Anche $(\mathbb{P}(\Gamma), \cap)$ \`e un monoide, con $\Gamma$ come elemento neutro, poich\'e $\forall \ A$, $\Gamma \cap A = A$. Abbiamo quindi due esempi di monoidi commutativi.

Fissato un insieme $S \subseteq \Gamma$ diverso da $\emptyset$, possiamo considerare il suo insieme delle parti $\mathbb{P}(S)$ e definire l'applicazione $f : \mathbb{P}(\Gamma) \to \mathbb{P}(S)$ tale che $f(A) = A \cap S$. 

Verifichiamo che questa applicazione \`e un morfismo di monoidi rispetto a $(\mathbb{P}(\Gamma), \cup)$ e $(\mathbb{P}(S), \cup)$. Dobbiamo dimostrare che $f( A \cup B) = f(A) \cup f(B)$. Infatti $f(A \cup B) = (A \cup B) \cap S = (A \cap S) \cup (B \cap S)$ per la propriet\`a distributiva, che \`e proprio $f(A) \cup f(B)$.

Inoltre l'applicazione conserva l'elemento neutro, poich\'e $f(\emptyset) = \emptyset \cap S = \emptyset$.

Verifichiamo che \`e un morfismo di monoidi anche rispetto $(\mathbb{P}(\Gamma), \cap)$ e $(\mathbb{P}(S), \cap)$. $f(A \cap B) = f(A) \cap f(B)$, infatti $(A \cap B) \cap S = (A \cap S) \cap (B \cap S)$ sempre per la propriet\`a distributiva. E anche in questo caso l'applicazione conserva l'elemento neutro, poich\'e $f(\Gamma) = \Gamma \cap S = S$, ed $S$ \`e proprio l'elemento neutro di $(\mathbb{P}(S), \cap)$.
\end{exmp}

\begin{exmp}
Sia $f : \mathbb{P}(\Gamma) \to \mathbb{P}(\Gamma)$ un'applicazione tale che $f(A) = \overline{A} = \{ x \in \Gamma : x \notin A\}$, ossia che associa ad $A$ il suo complementare $\overline{A}$. L'applicazione $f : (\mathbb{P}(\Gamma), \cup) \to (\mathbb{P}(\Gamma), \cap)$ \`e un morfismo di monoidi visto che verifica $f(A \cup B) = f(A) \cap f(B)$ per le leggi di De Morgan ($\overline{A \cup B} = \overline{A} \cap \overline{B}$) e $f(\emptyset) = \overline{\emptyset} = \Gamma$.

Possiamo considerare la stessa applicazione come un morfismo di monoidi da $f : (\mathbb{P}(\Gamma), \cap) \to (\mathbb{P}(\Gamma), \cup)$. Infatti $f(A \cap B) = \overline{A \cap B} = \overline{A} \cup \overline{B} = f(A) \cup f(B)$ e $f(\Gamma) = \overline{\Gamma} = \emptyset$.
\end{exmp}

\subsection{Congruenze\label{congruenze}}

\begin{defn}
Le congruenze sono relazioni d'equivalenza definite sulle strutture algebriche. Sia $\varepsilon$ una relazione d'equivalenza su $A$, con $(A, \cdot)$ monoide, si dice che $\varepsilon$ \`e una congruenza rispetto all'operazione $\cdot$ se, dati $a \ \varepsilon \ b$ e $c \ \varepsilon \ d$, ho che $a \cdot c \ \varepsilon \ b \cdot d$.
\end{defn}

Vuol dire che, dati $a \in [a]$ e $c \in [c]$, se $a \cdot c \in [a \cdot c]$ e ho una congruenza, allora $b \in [a]$ e $d \in [c]$ sono tali che $b \cdot d \in [a \cdot c]$. La congruenza fa s\`i che io possa definire operazioni sulle classi.

\begin{prop}
Riprendendo il teorema \ref{omomorfismo_monoidi}, abbiamo che considerato un morfismo di monoidi $f : (M, \cdot) \to (A, \ast)$, se definisco la relazione di equivalenza $\varepsilon_f$ tale che $x, y \in M$ sono $x \ \varepsilon_f \ y \Leftrightarrow f(x) = f(y)$, questa relazione di equivalenza \`e una congruenza.
\end{prop}
\begin{proof}\label{congruenza_monoidi}
$x \ \varepsilon_f \ y$, $z \ \varepsilon_f \ w \Rightarrow (x \cdot z) \ \varepsilon_f \ (y \cdot w)$.

Infatti $f(x \cdot z) = f(x) \ast f(z)$ e $f(y \cdot w) = f(y) \ast f(w)$. 
\end{proof}

Avevamo definito $\rho$ su $\mathbb{N} \times \mathbb{N}$, come $(a, b) \ \rho \ (c, d) \Leftrightarrow a+d = b+c$. Quindi a partire da $(M, \cdot) $ possiamo creare altri monoidi $(M^n, \cdot)$, ad esempio su $M^2 = M \times M$ in cui $(x, y) \cdot (z, t) = (x \cdot z, y \cdot t)$.

Ad esempio $(\mathbb{N}, +) \to (\mathbb{N} \times \mathbb{N}, +)$ in cui $(m, n) + (a, b) = (m+a, n+b)$ con l'elemento neutro $(0,0)$.

$\rho$ \`e una congruenza rispetto a + in $\mathbb{N} \times \mathbb{N}$. Inoltre, avendo visto che $\mathbb{N} \times \mathbb{N} / \rho = \mathbb{Z}$, abbiamo che $(\mathbb{Z}, +)$ \`e un monoide.

% WEB

\section{Gruppi}

\begin{defn}
$(G, \cdot)$ \`e un gruppo se:
\begin{description}
    \item[1G] $(G, \cdot)$ \`e un monoide
    \item[2G] $\forall \ a \in G $, $ \exists \ b \in G $ tale che $a \cdot b = 1_G$, con $b$ comunemente indicato come $a^{-1}$ e detto inverso di $a$.
\end{description}
\end{defn}

Possiamo definire un morfismo di gruppi. Un morfismo conserva strutture e propriet\`a, deve quindi essere un morfismo di monoidi che manda l'inverso nell'inverso.
\begin{defn}[Morfismo di gruppi]
Quindi $f : (G, \cdot) \to (G', \ast)$ \`e un morfismo di gruppi se:
\begin{enumerate}
    \item \`e un morfismo di monoidi
    \item $\forall a \in G f(a^{-1}) = (f(a))^{-1}$
\end{enumerate}
\end{defn}
\begin{prop}
Queste due propriet\`a sono la conseguenza di una sola, ossia che il morfismo conserva le operazioni. Infatti se $f(a \cdot b) = f(a) \ast f(b)$ allora sono vere tutte le propriet\`a.
\end{prop}
\begin{proof}
Un morfismo che conserva le operazioni manda le unit\`a nelle unit\`a: $f(1_G) = f(1_G \cdot 1_G) = f(1_G) \ast f(1_G)$. Essendo entrambi gruppi hanno l'inverso, quindi moltiplicando entrambi i lati per l'inverso di $f(1_G)$ abbiamo $(f(1_G))^{-1} \ast f(1_G) = (f(1_G))^{-1} \ast f(1_G) \ast f(1_G) \Rightarrow 1_{G'} = f(1_G)$.

Inoltre, se il morfismo conserva le operazioni manda gli inversi negli inversi, ossia $f(a^{-1}) = (f(a))^{-1}$. Infatti $f(a) \ast f(a^{-1}) = f(a \cdot a^{-1}) = f(1_G) = 1_{G'} = f(a) \ast (f(a))^{-1}$.
\end{proof}

\subsection{Sottogruppi}

\begin{defn}[Sottogruppo]
Partendo da $(G, \cdot)$ e scegliendo $S \subseteq G$ diverso da $\emptyset$, un sottogruppo $(S, \cdot)$ deve essere:
\begin{itemize}
    \item Chiuso: $\forall \ s, s' \in S$, $s \cdot s' \in S$
    \item Deve contenere l'unit\`a: $1_G \in S$ (quindi $S$ \`e un sottomonoide di $G$)
    \item Per essere anche un sottogruppo, $S$ deve essere chiuso rispetto agli inversi: $s \in S \Rightarrow s^{-1} \in S$.
\end{itemize}
\end{defn}
\begin{prop}
Condizione necessaria e sufficiente affinch\'e $(S, \cdot)$ con $S \neq \emptyset$ sia un sottogruppo del gruppo $(G, \cdot)$ \`e:
\[
a, b \in S \Rightarrow a^{-1} \cdot  b \in S
\]
\end{prop}
\begin{proof}
Dimostrare che \`e condizione necessaria \`e banale. Per definizione di sottogruppo $a^{-1}$ \`e in $S$, ed essendo chiuso $a^{-1} \cdot b \in S$.

Dobbiamo dimostrare che \`e sufficiente. $S \neq \emptyset$, quindi ha almeno un elemento $a \in S$. Prendiamo $b = a $, per la propriet\`a indicata sopra $a \cdot a^{-1} \in S \Rightarrow 1_G \in S$. Quindi $S$ contiene almeno l'elemento neutro.

Contiene l'inverso: $\forall \ x \in S $, $ x^{-1} \in S$ sempre per la propriet\`a sopra. Infatti prendendo $a = x$ e $b = 1_G$, $a^{-1} \cdot b \in S$ ossia $x^{-1} \cdot 1_G \in S \Rightarrow x^{-1} \in S$.

\`E chiuso: $\forall \ s, s' \in S \Rightarrow s \cdot s' \in S$. Abbiamo appena visto che $s \in S \Rightarrow s^{-1} \in S$, quindi per la solita propriet\`a ho che $(s^{-1})^{-1} \cdot s' \in S \Rightarrow s \cdot s' \in S$.
\end{proof}

% SONO ARRIVATO QUI

% $(\mathbb{Z}, +) \to (\mathbb{N} \times \mathbb{N}, +) / \rho$.

I sottogruppi di $(\mathbb{Z}, +)$ sono tutti e solo i gruppi $(k \mathbb{Z}, +)$. $(\mathbb{Z}, \cdot)$ non \`e un gruppo perch\'e non ha l'inverso per ogni elemento.

Dati $a, b \in G$, voglio conoscere l'inverso del prodotto $a \cdot b$, ossia $(a \cdot b)^{-1}$. Solitamente un gruppo $(G, \cdot)$ non \`e commutativo. Solo se il gruppo \`e commutativo ho che $(a \cdot b)^{-1} = a^{-1} \cdot b^{-1}$.

Consideriamo il gruppo simmetrico $(S_n, \circ)$, definito sull'insieme delle funzioni iniettive da un insieme con $n$ elementi in s\'e stesso con l'operazione di composizione. Non \`e un gruppo commutativo.

Prendiamo le due funzioni $\sigma$ e $\tau$ dal punto di vista dell'occupazione in figura \ref{fig:gruppo_simmetrico}. Se voglio trovare l'inverso $pi$ di $\sigma \circ \tau$ tale che $(\sigma \circ \tau) \circ \pi = i$ devo usare $\pi = (\tau^{-1} \circ \sigma^{-1})$.

\begin{figure}[ht]
\centering
\begin{tabular}{cccc}
\multicolumn{4}{c}{$\sigma$} \\
\hline
1 & 2 & 3 & 4 \\
2 & 3 & 1 & 4
\end{tabular} \qquad
\begin{tabular}{cccc}
\multicolumn{4}{c}{$\tau$} \\
\hline
1 & 2 & 3 & 4 \\
1 & 2 & 4 & 3
\end{tabular}

\begin{tabular}{cccc}
\multicolumn{4}{c}{$\sigma \circ \tau$} \\
\hline
1 & 2 & 3 & 4 \\
1 & 2 & 4 & 3 \\
2 & 3 & 4 & 1
\end{tabular} \qquad
\begin{tabular}{cccc}
\multicolumn{4}{c}{$\tau \circ \sigma$} \\
\hline
1 & 2 & 3 & 4 \\
2 & 3 & 1 & 4 \\
2 & 4 & 1 & 3
\end{tabular}
\caption{\label{fig:gruppo_simmetrico}Il gruppo simmetrico non \`e commutativo}
\end{figure}

Quindi, l'inverso del prodotto \`e il prodotto degli inversi scambiati di posto:
\[
(a \cdot b) \cdot (b^{-1} \cdot a^{-1}) = a \cdot (b \cdot b^{-1}) 
\cdot a^{-1} = a \cdot 1_G \cdot a^{-1} = 1_G
\]

Un'applicazione $f : G \to G'$ \`e un morfismo di gruppi se $\forall a, b \in G$ ho che $f(a \cdot b) = f(a) \ast f(b) \Rightarrow f(1_G) = 1_{G'}$ e $f(a^{-1}) = (f(a))^{-1}$.

\begin{exmp} 
La funzione $\log : (\mathbb{R}^{+}, \cdot) \to (\mathbb{R}, +)$ \`e un morfismo di gruppi, perch\'e $\log(a \cdot b) = \log(a) + \log(b)$.

Anche la funzione $\exp : (\mathbb{R}, +) \to (\mathbb{R}^{+}, \cdot)$ \`e un morfismo di gruppi, infatti  $\exp(a + b) = \exp(a) \cdot \exp(b)$.

Anche l'iterazione della somma \`e un morfismo di gruppi. $f_n : (\mathbb{Z}, +) \to (\mathbb{Z}, +) $, infatti $\forall \ z \in \mathbb{Z}$, $f_n(z) = n \cdot z$
\end{exmp}

\subsection{Nucleo di un morfismo di gruppi}

Ogni morfismo di gruppi $f$ individua due sottogruppi:
\begin{itemize}
    \item $Im_f \subseteq G'$
    \item $\ker f \subseteq G$. Diversamente dalle definizioni gi\`a viste, in questo caso il nucleo $\ker f = \{ u \in G : f(u) = 1_{G'}\}$ \`e una classe, ossia $\ker f \in G / \varepsilon_f$
\end{itemize}

Con i monoidi avevamo una struttura associativa $(M, \cdot)$ contenente l'unit\`a $1_M$. Il $\ker f$ l'abbiamo chiamato ``quoziente'', ossia:
\[
\ker f = M / \varepsilon_f
\]

\begin{figure}[ht]
\centering
\begin{tikzpicture}
  \node (A) {$A$};
  \node (f) [right of=A, node distance=2cm] {$f$};
  \node (B) [right of=f, node distance=2cm] {$B$};
  \node (a) [below of=A, node distance=1cm] {$a$};
  \node (b) [below of=a, node distance=1cm] {$b$};
  \node (c) [below of=b, node distance=1cm] {$c$};
  \node (d) [below of=c, node distance=1cm] {$d$};
  \node (e) [below of=d, node distance=1cm] {$e$};
  \node (1) [below of=B, node distance=1cm] {$1$};
  \node (2) [below of=1, node distance=1cm] {$2$};
  \node (3) [below of=2, node distance=1cm] {$3$};
  \node (4) [below of=3, node distance=1cm] {$4$};
  \node (5) [below of=4, node distance=1cm] {$5$};
  \node (6) [below of=5, node distance=1cm] {$6$};
  \node (7) [below of=6, node distance=1cm] {$7$};
  \path[->]  (a) edge node {} (3)
            (b) edge node {} (3)
            (c) edge node {} (4)
            (d) edge node {} (4)
            (e) edge node {} (7)
            ;
\end{tikzpicture}
\caption{\label{fig:esempio_funzione}$\ker f = \{ \{a, b \}, \{ c, d \}, \{ e \}\}$ }
\end{figure}

Nel caso in figura \ref{fig:esempio_funzione} $\ker f = \{ \{a, b \}, \{ c, d \}, \{ e \}\}$. Ho \textit{bisogno} di sapere quali classi ci sono per ricostruire la funzione. Per conoscere la $f$ devo sapere tutti i blocchi della partizione, non posso ricostruire gli altri blocchi da un blocco solo.

Con i gruppi non \`e cos\`i. Mi basta la classe degli elementi che vanno nell'unit\`a. Se conosco questa classe le conosco tutte. Infatti fissato il $\ker f$ conosco tutti gli elementi che hanno la stessa immagine di $a$, ossia $a \cdot \ker f = [a]$.

Abbiamo un morfismo di gruppi $ f : (G, \cdot ) \to (G', \ast)$. Dimostriamo intanto che il nucleo \`e un sottospazio, ossia un sottogruppo.

$\ker f = [1_G]$ conosco tutti gli elementi che finiscono nell'unit\`a di $G'$

CNES afficnhe un sottoinsieme sia un sottogruppo \`e che se $a, b \in S \Rightarrow a^{-1} b \in S$.

Prendiamo $u, v \in \ker f$. La tesi \`e che $u^{-1} \cdot v \in \ker f$. Quindi $f ( u^{-1} \cdot v ) = 1_{G'}$.
\[
f( u^{-1} \cdot v ) = f(u^{-1}) \ast f(v) = f(u)^{-1} \ast f(v)
\]
Ma $f(u)^{-1} = 1_{G'}$, quindi ho:
\[
f(u)^{-1} \ast f(v) = 1_{G'} \ast 1_{G'} = 1_{G'}
\]

\subsection{Teorema di omomorfismo per i gruppi}

Sia $f : (G, \cdot) \to (G', \ast)$ un morfismo di gruppi, allora $\varepsilon_f$ \`e una congruenza e il gruppo $G / \varepsilon_f$ \`e isomorfo al gruppo $(Im_f, \ast)$. Ogni elemento $[a] \in G / \varepsilon_f$ \`e del tipo $a \cdot \ker f$.
\[
F : (G / \varepsilon_f, \cdot) \to (Im_f, \ast)
\]
\[
\forall [a] \in G / \varepsilon_f [a] = a \cdot \ker f
\]
Dimostrazione:
$b \in [a] \Rightarrow f(a) = f(b)$ per definizione hanno la stessa immagine tramite il morfismo.

Quindi moltiplico entrambi i membri per $f(a)^{-1}: f(a)^{-1} \ast f(b) = 1_{G'} \Rightarrow 1_{G'} = f(a)^{-1} \ast f(b) = f(a^{-1} \cdot b)$

Quindi $u = (a^{-1} \cdot b ) \in \ker f e b = a \cdot u = a \cdot (a^{-1} \cdot b)$

Viceversa, dobbiamo prendere $b \in a \cdot \ker f \Rightarrow b = a \cdot u$. Ha per forza la stessa immagine di $a$, infatti $f(b) = f(a \cdot u) = f(a) \cdot f(u) = f(a) \cdot 1_G = f(a)$. Segue che $b$ \`e nella classe di $a, b \in [a]$.

Non essendo commutativo ho che $b = a \cdot u = v \cdot a$. Posso vedere $b \in a \cdot \ker f$ sia in $\ker f \cdot a$.

La prima \`e la classe laterale sinistra, la seconda \`e la classe laterale destra.
\[
\forall a \in G a \cdot ker f = \ker f \cdot a
\]
\[
b = u \cdot a
\]
\[
f(b) = f(u) \cdot f(a) = 1_{G'} \cdot f(a)
\]
Proiezioni. La seguente \`e la prima proiezione
\[
p_1 : \mathbb{R} \times \mathbb{R} \to \mathbb{R}
\]
\[
p_1 (x, y) = x
\]
\[
p_1 : (\mathbb{R} \times \mathbb{R}, +) \to (\mathbb{R}, +)
\]
Qual \`e l'immagine $Im_{p_1}$? $Im_{p_1} = \{ r \in \mathbb{R} : \exists (x, y) p_1 (x, y) = r \}$. In questo caso l'immagine \`e tutto $\mathbb{R}$. Infatti $\forall r \in \mathbb{R} p_1(r, 0) = r$.

Adesso troviamo il nucleo.
\[
\ker f = \{ (x, y) \in \mathbb{R} \times \mathbb{R} : p_1(x, y) = 0 \} = \{ (0, y) : y \in \mathbb{R} \}
\]
L'unit\`a di $G 1_G = (0, 0) \in \ker p_1$

0 \`e l'elemento neutro di $(\mathbb{R}, +)$.
\[
(2, 3) \in \mathbb{R} \times \mathbb{R}, p_1(2, 3) = 2
\]
\[
[(2, 3)] = \{ (x, y) \in \mathbb{R} \times \mathbb{R} : f(x, y) = 2\}
\]
Applicando il teorema di omomorfismo visto prima vediamo subito che \`e:
\[
[(2,3)] = (2, 3) + \ker p_1 = \{ (2, 3) + (0, y) = (2, y + 3) \}
\]
Altro esempio:
$\mathbb{R}[x] =$ insieme dei polinomi in una indeterminata $x$

un polinomio \`e una espressione formale del tipo $a_0 + a_1 x + \dots a_n x^n$ dove $a_n \neq 0$ e $n$ si dice grado del polinomio.

C'\`e una differenza fra $x \to$ indeterminata e $x \to $ variabile. L'indeterminata fa parte dei polinomi, e vuol dire che $x$ \`e un simbolo. Variabile vuol dire che \`e un elemento di un insieme, ossia $x \in E$. In genere si confonde indeterminata con variabile, perch\'e quando si parla di polinomi la $x$ \`e indeterminata, ma ogni polinomio individua una funzione polinomiale. $p : R \to R a \mapsto p(a)$
\[
p(x) = 1 + 2x
\]
individua la funzione polinomiale $p : R \to R \forall a \in R$ associa $1 + 2 \cdot a = p(a)$. Nel caso dei numeri reali, questa funzione \`e una biezione. Se due polinomi hanno la stessa funzione polinomiale, allora sono lo stesso polinomio. Non \`e vero se prendo altri insiemi.

Per fare i polinomi bisogna avere un campo.

Se prendo $Z_2 = {0,1}$, ossia i resti della divisione per 2. Possiamo definire due operazioni, di somma e di prodotto.

\begin{tabular}{c|cc}
+ & 0 & 1 \\
\hline
0 & 0 & 1 \\
1 & 1 & 0
\end{tabular}

$Z / \equiv_2$, congruenza modulo 2.

\begin{tabular}{c|cc}
$\cdot$ & 0 & 1 \\
\hline
0 & 0 & 0 \\
1 & 0 & 1
\end{tabular}

Un campo \`e un gruppo rispetto a $+$ e un gruppo rispetto a $\cdot$ senza elemento neutro (0).

Posso creare i polinomi a coefficienti in $Z_2$, ossia $Z_2[x]$, ad esempio $1 + x$. La funzione polinomiale rispetto a questo polinomio \`e:
\begin{itemize}
    \item per $x = 0 \to p(0) = 1$
    \item per $x = 1 \to p(1) = 0$
\end{itemize}
Ma dato il polinomio $1 + x^2$ ho:
\begin{itemize}
    \item per $x = 0 \to p(0) = 1$
    \item per $x = 1 \to p(1) = 0$
\end{itemize}
Sono quindi due polinomi diversi che hanno la stessa funzione polinomiale.

Due polinomi sono uguali se hanno tutti i coefficienti uguali.

$(R[x], +)$ \`e un gruppo commutativo. Come funziona il +? Sommo i coefficienti. L'elemento neutro \`e il polinomio nullo, ossia il polinomio con tutti i coefficienti uguali a 0. Si indica con $\underline{0}$. Il polinomio nullo ha grado -1.

Posso definire un morfismo di gruppi su tutta sta merda.

Prendo tutti i polinomi di grado minore o uguale a due, e li indico $R_2[x]$.

Prendo l'applicazione $f : (R_2[x], +) \to (R^2, +)$ definito come segue:
\[
f(a_0 + a_1 \cdot x + a_2 \cdot x^2) = (a_2, a_1 + a_2)
\]
Quindi $1 + 2x \mapsto (0, 2)$.

Qual \`e l'immagine di questa $f$? 
\[
Im_f = \{ (r, s) \in R^2 : \exists p(x) t.c. f(p(x)) = (r,s)\} = R^2
\]
\[
(r, s) = f(0 + (s - r) x + r x^2)
\]
Troviamo il nucleo.
\[
\ker f = \{ p(x) \in R^2[x] : f(p(x)) = (0, 0)\}
\]
Quindi sono tutti i polinomi di grado 0 pi\`u il polinomio nullo, dovendo avere $a_2 = 0$ e $a_1 = 0$.

Tutti i polinomi $[p(x)] = a_0 + a_1 x + a_2 x^2$ devono potersi scrivere come $p(x) + \ker f$.
\[
f(p(x)) = (a_2, a_1 + a_2)
\]
\[
p(x) + \ker f
\]
\[
q(x) \in [p(x)] sono q(x) = p(x) + a_0
\]
con $a_0 \in \ker f$

Esercizio:

$(G, \cdot)$, $(G', \ast)$, poi abbiamo il morfismo $f : (G, \cdot) \to (G', \ast)$
\[
\ker f \subseteq G
\]
\[
Im_f \subseteq G'
\]
Caratterizzano il morfismo, questi due gruppi. f \`e un morfismo iniettivo (monomorfismo) $\Leftrightarrow \ker f = \{ 1_G\}$. Il morfismo \`e suriettivo (epimorfismo) $\Leftrightarrow$ solo se $Im_f = G'$.

Il primo caso \`e evidente, il $\ker f$ ha solo un elemento quindi la classe degli elementi con la stessa immagine $[a] = a \cdot \ker f$ ha un solo elemento, ossia $a$. Si pu\`o anche dimostrare direttamente.

$f$ \`e iniettiva per ipotesi. La tesi \`e che il nucleo \`e costituito da un solo elemento $\ker f = \{ 1_G \} $ ossia $f(1_G) = 1_{G'}$

Viceversa se $\ker f = \{ 1_G \} \Rightarrow f(a) = f(b) \Rightarrow f(a) \cdot f(b)^{-1} = 1_{G'} = f(a \cdot b^{-1}) = 1_{G'} \Rightarrow a \cdot b^{-1} = 1_G \Rightarrow a = b$

Se prendiamo $S$ sottogruppo di $G$ possiamo considerare la classe laterale destra $a S$ e la classe laterale sinistra $S a$. Lo vediamo la prossima volta. 

\subsection{Teorema di omomorfismo per i gruppi}

Dato un morfismo $f : (G, \cdot) \to (G', \ast)$, allora 
\begin{enumerate}
    \item $\varepsilon_f$ individuata da $f$ ($x \ \varepsilon_f \ y \Leftrightarrow f(x) = f(y)$) \`e una congruenza
    \item Il gruppo $(G / \varepsilon_f, \cdot)$ \`e isomorfo al sottogruppo $(Im_f, \ast)$ di $(G', \ast)$. Questo vale per ogni struttura algebrica.
\end{enumerate}

$f$ individua due sottogruppi, $\ker f = \{ u \in G : f(u) = 1_{G'} \} \le G$, $Im_f = \{ x' \in G' : \exists x \in G f(x) = x'\} \le G'$. 
\[
\forall a \in G [a] = a \cdot \ker f = \ker f \cdot a
\]
\begin{prop}
Tutte le classi hanno la stessa cardinalit\`a $|[a]| = |a \cdot \ker f| = |\ker f|$
\end{prop}
\begin{proof}
Bisogna far vedere che esiste una corrispondenza biunivoca.
\[
\forall a \in G \varphi_a : \ker f \to a \cdot \ker f
\]
\[
\forall u \in \ker f \varphi_a (u) = a \cdot u \in a \cdot \ker f
\]
$\varphi_a$ \`e biunivoca. 

$\varphi_a$ \`e iniettiva, infatti $\forall u$, $v \in \ker f $ tali che $\varphi_a (u) = \varphi_a (v)$, ho che a $\cdot u = a \cdot v \Rightarrow$ essendo in un gruppo $a$ ha l'inverso, quindi $a^{-1} \cdot a \cdot u = a^{-1} \cdot a \cdot v \Rightarrow u = v$.
\end{proof}

Vediamo cosa succede se prendiamo un sottogruppo qualunque.

Sia $S \le G$ un sottogruppo qualunque di $G$. Prendo $a \in G$ e moltiplico tutti gli elementi di $S$ per $a$, ossia faccio $a \cdot S$ e $S \cdot a$.

$a \cdot S = \{ x \in G : x = a \cdot s con s \in S \}$ \`e la classe laterale sinistra di $S$

$S \cdot a = \{ x \in G : x = s \cdot a con s \in S \}$ \`e la classe laterale destra di $S$

Per lo stesso motivo di prima tutte le classi laterali hanno la stessa cardinalit\`a di $S |a \cdot S| = |S| = |S \cdot a| \forall a \in G$

Prendiamo l'insieme di sottoinsiemi sinistri
\[
\{ a \cdot S\}_{a \in G} a \cdot S \subseteq G
\]
$a \cdot S$ non \`e un sottogruppo perch\'e non contiene l'unit\`a, ma \`e un sottoinsieme. L'insieme delle classi sinistre \`e una partizione di $G$.
\begin{enumerate}
    \item $\bigcup_{a \in G} a \cdot S = G$
    \item $a \cdot S \neq \emptyset$, infatti necessariamente $a \in a \cdot S$, essendo $a = a \cdot 1_G$ e $1_G \in S$
    \item $a \cdot S \cap a \cdot S \neq \emptyset \Rightarrow a \cdot s = b \cdot S$
\end{enumerate}
Dimostriamo il punto tre.

$a \cdot S \cap b \cdot S \neq \emptyset \Rightarrow a \cdot S = b \cdot S$

\[
c \in a \cdot S \cap b \cdot S
\]
\[
c = a \cdot s = b \cdot v con s, v \in S
\]
\`e l'ipotesi

La tesi \`e:
$x \in a \cdot S \Rightarrow = a \cdot u$ con $u \in S$
Per ipotesi ho $a = c \cdot s^{-1} \Rightarrow x = c \cdot s^{-1} \cdot u$, ma sempre per ipotesi ho che $x = b \cdot v \cdot s^{-1} \cdot u$. Quindi $x \in b \cdot S$, avendo che $(v \cdot s^{-1} \cdot u) \in S$.

Se $\{a \cdot S\}_{a \in G}$ \`e una partizione di $G$, individua in $G$ una relazione di equivalenza che indichiamo con $\lateralsx{S}$ (perch\'e che $S$ \`e una classe laterale sinistra).

Dico che due elementi sono equivalenti se sono nello stesso blocco. Ossia dati $x, y \in G$ dico $x \lateralsx{S} y \Leftrightarrow \exists a \in G x, y \in a \cdot S \Leftrightarrow x = a \cdot s$ e $y = a \cdot v$ con $s, v \in S$. Si pu\`o semplificare ulteriormente questa definizione, perch\'e se un elemento $x$ \`e nella classe posso prendere $x$ come rappresentante, e quindi dire che $x \in y \cdot S$ o che $y \in x \cdot S$ o che $x \cdot S = y \cdot S$ (sono tutte definizioni equivalenti).

Quindi posso scrivere $x \in y \cdot S$ come $x = y \cdot s$ con $s \in S$.
\[
s = y^{-1} \cdot x \in S
\]
Qual \`e la differenza con il nucleo? Nel nucleo le classi laterali coincidono, in generale no. Le classi laterali hanno la stessa cardinalit\`a ma non sono identiche.

Le due relazioni di equivalenza destra e sinistra (simboli qui) non sono uguali, e non sono congruenze.
\[
a \cdot S \neq S \cdot a
\]
Nel caso del nucleo invece abbiamo che $a \cdot \ker f = \ker f \cdot a$, e la relazione di equivalenza destra e sinistra \`e una sola, ossia $\varepsilon_f$, ed \`e una congruenza.

Facciamo un esempio. Consideriamo $S_4$

$(G, \cdot)$ e $a \in G$, possiamo indicare con $< a > =$ sottogruppo generato da $a \in G$ costitiuto da tutte le potenze generate da $a$. Per le propriet\`a delle potenze \`e un sottogruppo, infatti contiene $1_G = a^{0}$.
\[
S_4
\]
\[
< a > = \{ a^{z} : z \in \mathbb{Z} \}
\]
Contiene anche l'inverso di $a^{n}$, ossia $a^{-n}$

Prendiamo il sottogruppo generato da questa permutazione, e tutte le potenze generate da questa $\sigma$:

\begin{tabular}{cccc}
\multicolumn{4}{c}{$\sigma$} \\
1 & 2 & 3 & 4 \\
2 & 3 & 1 & 4 
\end{tabular}

\[
\sigma^{0} = id
\]
\[
\sigma^{1} = \sigma
\]
$\sigma^{2}$ cosa \`e?

\begin{tabular}{cccc}
\multicolumn{4}{c}{$\sigma^{2}$} \\
1 & 2 & 3 & 4 \\
2 & 3 & 1 & 4 \\
3 & 1 & 2 & 4
\end{tabular}

Se poi faccio $\sigma^{3}$ riottengo l'identit\`a.

\begin{tabular}{cccc}
\multicolumn{4}{c}{$\sigma^{3}$} \\
1 & 2 & 3 & 4 \\
3 & 1 & 2 & 4 \\
1 & 2 & 3 & 4 
\end{tabular}

Quindi il gruppo $H = < \sigma > = \{ 1 , \sigma, \sigma^{2} \}$. L'inversa di $\sigma$ \`e $\sigma^{2}$. $H$ \`e un gruppo finito di ordine 3.

Facciamo la relazione di equivalenza e la classe laterale.

Prendiamo due elementi equivalenti, $\mu$  e $\tau \Leftrightarrow \sigma \tau^{-1} \in H$. Quindi $\mu \tau^{-1} = \rho \in H$, quindi o l'identit\`a, o $\sigma$ o $\sigma^{2}$.

\begin{tabular}{cccc}
\multicolumn{4}{c}{$\tau$} \\
1 & 2 & 3 & 4 \\
2 & 1 & 4 & 3
\end{tabular}

\begin{tabular}{cccc}
\multicolumn{4}{c}{$\sigma \cdot \tau$} \\
1 & 2 & 3 & 4 \\
2 & 1 & 4 & 3 \\
3 & 2 & 4 & 1
\end{tabular}

Quindi $\mu$ \`e:

\begin{tabular}{cccc}
\multicolumn{4}{c}{$\mu$} \\
1 & 2 & 3 & 4 \\
3 & 2 & 4 & 1
\end{tabular}

Prendiamo $\tau'$

\begin{tabular}{cccc}
1 & 2 & 3 & 4 \\
4 & 3 & 2 & 1
\end{tabular}

Applichiamo $\sigma$

\begin{tabular}{cccc}
\multicolumn{4}{c}{$\sigma \cdot \tau'$} \\
1 & 2 & 3 & 4 \\
4 & 3 & 2 & 1 \\
4 & 1 & 3 & 2
\end{tabular}

Otteniamo $\mu'$ equivalente a $\tau'$

\begin{tabular}{cccc}
\multicolumn{4}{c}{$\mu'$} \\
1 & 2 & 3 & 4 \\
4 & 1 & 3 & 2
\end{tabular}

% Dobbiamo far vedere ora che \mu, \tau, \mu' e \tau' sono nella stessa classe.

Siamo nella classe destra, avendo fatto $\sigma \tau$ (va vista come composizione).

La congruenza mi d\`a modo di definire il prodotto fra classi. 

Dati $a, b$ nella classe 1, dati $a', b'$ nella classe 2. Le classi sono definite sulla struttura $(A, \cdot)$. Vogliamo definire il prodotto fra classi, ossia $[1] \cdot [2] = [3]$. 

La congruenza fa in modo che comunque prendo i rappresentanti i prodotti vanno sempre nella stessa classe. Quindi $a \cdot a'$ e $b \cdot b' \in [3]$.

Possiamo vedere che $\tau$ per $\tau'$ e $\mu$ per $\mu'$ non vanno nella stessa classe.

Che ordine ha il sottogruppo $H$? Lo possiamo dire per il teorema di Lagrange.

Teorema di Lagrange
$(G, \cdot)$ gruppo finito, la cardinalit\`a di $G$ si dice ordine.

Prendiamo $|G| = n$, allora se $H$ \`e un sottogruppo di $G$, l'ordine di $G$ \`e diviso dall'ordine di $H$
\[
\frac{|G|}{|H|}
\]
Questo intero \`e detto indice di $H$.

Un gruppo di ordine un numero primo ha due sottogruppi.

$G \to \{ a H\}_{a \in G}$
\`e una partizione di $G$
\[
|a H| = |H|
\]
\[
|G / \lateralsx{H}| = \frac{|G|}{|H|}
\]
\begin{defn}
un sottogruppo $N$ di $G$ si dice normale se $\forall a \in G$ $a N = N a$, ossia ogni classe laterale destra \`e uguale alla classe laterale sinistra. I nuclei dei morfismi sono sottogruppi normali.
\end{defn}
CNES affinch\'e $N$ sia normale \`e che $\forall a \in G$ e $\forall u \in N$, $a \cdot u \cdot a^{-1} \in G$. Deriva banalmente da $a \cdot N = N \cdot a$

CNES affinch\'e $N$ sia normale \`e che $\lateraldx{N}$ (o $\lateralsx{N}$) \`e una congruenza.

Tutti i nuclei dei morfismi sono sottogruppi normali.

Ogni permutazione \`e una biezione che pu\`o essere indicata sia dal punto di vista dell'occupazione sia dal punto di vista della distribuzione (come parola).

\[
\sigma \in S_8
\]

\begin{tabular}{*{8}{c}}
\multicolumn{8}{c}{$\sigma$} \\
1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 \\
1 & 7 & 4 & 6 & 5 & 2 & 3 & 8
\end{tabular}

O come parola:
\[
17465238
\]
Possiamo indicare le permutazioni come composte di cicli.
\[
\sigma = (1) (2 7 3 4 6) (5) (8)
\]
$(2 7 3 4 6)$ significa che il 2 va nel 7, il 7 nel 3, il 3 nel 4 e il 4 nel 6.

$\mu (3 1) (5 4 2) (8 7 6)$ \`e un prodotto di cicli. Corrisponde alla permutazione:

\begin{tabular}{*{8}{c}}
\multicolumn{8}{c}{$\mu$} \\
1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 \\
3 & 5 & 1 & 2 & 4 & 8 & 6 & 7
\end{tabular}

Le permutazioni vengono rappresentate come prodotti di cicli. I cicli di lunghezza 1 non vengono scritti, visto che ogni elemento va a finire in s\'e stesso. Quindi $\sigma = (1) (2 7 3 4 6) (5) (8)$ posso scriverla come $\sigma = (2 7 3 4 6)$.

Data una permutazione
\[
\sigma \in S_n 
\]
Definita su
\[
[n] = \{1 \dots n\}
\]
Ho la relazione di equivalenza sui cicli
$x equiv_sigma y \Leftrightarrow \exists n \in \mathbb{N} $ t.c. $ y = \sigma^{n} (x)$

\textbf{Esercizio:} questa \`e una relazione di equivalenza che divide $[n]$ in classi di equivalenza. Le classi sono i cicli.

Ad esempio, nel caso del $\sigma$ di prima, $7 = \sigma(2)$, $3 = \sigma^{2}(2)$, $4 = \sigma^{3} (2)$, $6 = \sigma^{4} (2)$.

$x$ \`e equivalente a tutti gli elementi $\sigma (x), \sigma^{2} (x), \dots \sigma^{t}(x)$ fino alla $t$-esima permutazione che torna in $x$ (altrimenti il ciclo sarebbe infinito).
\[
\mu_x : [n] \to [n]
\]
Definita come:
\[
\mu_x (y) =
\begin{cases}
y se y \notin [x] \\
\sigma(y) se y \in [x]
\end{cases}
\]
Quindi $\mu_x$ si comporta come $\mu$ nella partizione individuata da $x$, e tutti gli altri restano fissi.

Quindi $\mu_3$ \`e:

\begin{tabular}{*{8}{c}}
\multicolumn{8}{c}{$\mu_3$} \\
1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 \\
3 & 2 & 1 & 4 & 5 & 6 & 7 & 8
\end{tabular}
\[
\mu_3 = \mu_1
\]
$\mu_5 = \mu_4 = \mu_2$ si comporta come $\mu$ solo sulla partizione individuata da 5:

\begin{tabular}{*{8}{c}}
\multicolumn{8}{c}{$\mu_5$} \\
1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 \\
1 & 5 & 3 & 2 & 4 & 6 & 7 & 8
\end{tabular}

$\mu_6 = \mu_7 = \mu_8$ si comporta come $\mu$ solo sulla partizione individuata da 6:

\begin{tabular}{*{8}{c}}
\multicolumn{8}{c}{$\mu_6$} \\
1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 \\
1 & 2 & 3 & 4 & 5 & 8 & 6 & 7
\end{tabular}

$\mu$ come prodotto di cicli si scrive come prodotto di cicli, ossia posso comporre $\mu_3$, $\mu_5$ e $\mu_6$ per ottenere $\mu$.

\begin{tabular}{*{9}{c}}
\multicolumn{9}{c}{$\mu = \mu_3 \circ \mu_5 \circ \mu_6$} \\
1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & \\
3 & 2 & 1 & 4 & 5 & 6 & 7 & 8 & $\mu_3$ \\
3 & 5 & 1 & 2 & 4 & 6 & 7 & 8 & $\mu_5$ \\
3 & 5 & 1 & 2 & 4 & 8 & 6 & 7 & $\mu_6$
\end{tabular}

Questo si chiama rappresentazione delle permutazioni come cicli disgiunti.

Non si possono omettere le parentesi, o si avrebbe una parola.

Con $\mathbb{N}$ possiamo trovare una rappresentazione standard (o canonica) dei cicli, senza parentesi.

Consideriamo i cicli (3 1 2) 5 (7 8) (4 6)

\begin{enumerate}
    \item descrivo i cicli partendo dall'elemento maggiore:
    (3 1 2) 5 (8 7) (6 4)
    \item ordino in maniera crescente in base al primo elemento
    (3 1 2) 5 (6 4) (8 7)
    \item ora posso togliere le parentesi, perch\'e so che i cicli finiscono al primo elemento non decrescente
    3 1 2 5 6 4 8 7
\end{enumerate}

Rappresentazione canonica o standard di una permutazione di $[n]$.

Trasposizioni = permutazioni che scambiano due elementi. Quindi hanno un solo ciclo di lunghezza 2 e tutti gli altri di lunghezza 1.

\begin{theorem}
Ogni permutazione si pu\`o scrivere come un prodotto di trasposizioni. Il numero di trasposizioni varia.
\end{theorem}

Se $\sigma$ si esprime come un prodotto di un numero pari di trasposizioni, allora ogni altro prodotto di trasposizioni che mi d\`a $\sigma$ ha un numero pari di trasposizioni. Vale anche con i dispari.

\begin{defn}
Una permutazione \`e pari se si esprime come prodotto di un numero pari di trasposizioni, dispari se si esprime come prodotto di un numero dispari di trasposizioni.
\end{defn}

$A_n =$ insieme delle permutazioni pari $\subseteq S_n$

\`E un sottogruppo di $S_n$?

$1 \in A_n$ (contiene l'unit\`a)

Deve essere un gruppo rispetto alle permutazioni. $\sigma, \mu \in A_n \Rightarrow (\sigma \cdot \mu) \in A_n$.

Allo stesso modo $\sigma^{-1} \in A_n$. Come si ottiene $\sigma^{-1}$?

$\sigma = \tau_{1} \dots \tau_{n}$ con $n = 2t$ e $\tau_i$ una trasposizione. L'inverso di una trasposizione \`e se stessa, ossia $\tau^{-1} = \tau$, quindi:
\[
\sigma^{-1} = \tau_n \dots \tau_1
\]
$A_n$ \`e quindi un sottogruppo di $S_n$ e si chiama gruppo alterno di ordine \label{gruppo_alterno} $n$.

L'insieme delle permutazioni dispari non \`e un sottogruppo di $S_n$ perch\'e il prodotto di due permutazioni dispari \`e una permutazione pari.

Il numero delle permutazioni dispari \`e uguale al numero delle permutazioni pari.
\[
|A_n| = \frac{n!}{2}
\]
\textbf{Esercizio:} trovare la corrispondenza biunivoca fra le permutazioni pari e le permutazioni dispari.

Trovare $F : A_n \to P_n$ e $F^{-1} : P_n \to A_n$, con $P_n$ ad indicare l'insieme delle permutazioni dispari.

Come conseguenza abbiamo quanto detto sopra, ossia siccome la cardinalit\`a dell'insieme delle permutazioni ha cardinalit\`a $n!$ la cardinalit\`a di $A_n$ \`e $\frac{n!}{2}$

L'indice di $A_n$ \`e:
\[
\frac{|S_n|}{A_n} = 2
\]
$A_n$ \`e un sottogruppo normale.

$A_n$ \`e il nucleo del morfismo $f : S_n \to \mathbb{Z}_2$:
\[
f (\sigma) = 
\begin{cases}
0 \text{ se } \sigma \text{ \`e pari} \\
1 \text{ se } \sigma \text{ \`e dispari} 
\end{cases}
\]
\`E un morfismo perch\'e 1 per 1 va in 0 e 0 per 0 va in 0, ossia una permutazione pari per una pari va in una pari, e una permutazione dispari per una dispari va in una pari.

% SONO ARRIVATO QUI A SISTEMARE

A_n = gruppo alterno su n elementi. \`E un sottogruppo di S_n.

|A_n| = \frac{n!}{2}

Esercizio: dimostrarlo

|S_n| = n!

A_n \`e il gruppo di permutazioni pari, ossia l'insieme di permutazioni esprimibili come il prodotto di un numero pari di trasposizioni.

Una trasposizione ha \textit{un} ciclo di lunghezza 2 e tutti gli altri di lunghezza 1.

D_n = insieme delle permutazioni dispari.

C'\`e una biezione F : A_n \to D_n

\forall \sigma \in A_n F(\sigma), come la definisco la biezione?

Prendiamo [n] - \{ 1 \dots n \} sui primi n numeri naturali. \sigma \`e una permutazione pari sui primi n numeri naturali. Per rendere \sigma dispari, la moltiplico per un'altra trasposizione.

F(\sigma) = (1 2) \cdot \sigma \in D_n

F \`e biunivoca. Deve esistere F^{-1} : D_n \to A_n

D_n(\sigma) = (1 2) \cdot \sigma

(F F^{-1}) (\delta) = F((1 2) \delta) = (1 2) \cdot (1 2) \delta = \delta

(F^{-1} F) (\sigma) = F^{-1}((1 2) \sigma) = (1 2) \cdot (1 2) \cdot \sigma = \sigma

Esercizio 2: ogni permutazione si esprime come prodotto di trasposizioni.

Prendiamo una permutazione \sigma. Abbiamo dimostrato che le permutazioni si possono esprimere come prodotto di cicli.

\sigma = \mu_1 \cdot \mu_2 \dots \mu_t

\sigma \`e il prodotto di t permutazioni. Ciascuna \mu_i \`e una permutazione k_i-ciclica, ossia \mu_i ha un solo ciclo di lunghezza i, tutti gli altri cicli sono di lunghezza 1.

1 2 3 4 5 6 7
2 4 1 3 6 5 7

(1 2 4 3) (5 6) = \mu_1 \cdot \mu_2

\mu_1 = (1 2 4 3)

\mu_2 = (5 6)

Quindi basta dimostrare che ogni permutazione k ciclica si pu\`o esprimere come un prodotto di trasposizioni.

Per scrivere una permutazione k ciclica come prodotto di trasposizioni, accoppio gli elementi a due a due.

(1 2 4 3) = (1 2) (2 4) (4 3)

1 2 3 4 5 6 7
1 2 4 3 5 6 7
1 4 2 3 5 6 7
2 4 1 3 5 6 7

L'ordine \`e l'ordine di composizione delle funzioni.

Quindi, dato un ciclo \mu_i = a_1 \dots a_t, lo scrivo come prodotto di trasposizioni:

\mu_i = (a_1 a_2) (a_2 a_3) \dots (a_{t-1} a_t)

Quindi ogni permutazione k_i-ciclica ha parit\`a uguale alla parit\`a di k_i - 1.

Quindi la parit\`a di \sigma = \mu_1 \cdot \mu_2 \dots \mu_t \`e:

\sum_{i = 1}^{t} (k_i - 1) = t + \sum_{i = 1}^{t} k_i

L'ordine di una permutazione \sigma \`e la cardinalit\`a del sottogruppo generato da \sigma, < \sigma >, ossia tutte le potenze di \sigma. Se prendo una permutazione \mu k-ciclica, l'ordine di \mu \`e k.

Una trasposizione ha ordine 2. < (5 6) > = \{ 1, (5 6) \}

Il sottogruppo generato da < (1 2 4 3) > = \{ 1, (1 2 4 3), (1 4) (2 3), (1 3 4 2) \}

(1 2 4 3)^2
1 2 3 4
2 4 1 3
4 3 2 1

Quindi (1 2 4 3)^2 = (1 4) (2 3)

(1 2 4 3)^2
1 2 3 4
2 4 1 3
4 3 2 1
3 1 4 2

Quindi (1 2 4 3)^3 = (1 3 4 2)

k \`e quindi \inf (t : \sigma^t = 1 con t \neq 0), \`e il pi\`u piccolo intero t per cui \sigma^t \`e l'identit\`a.

Se in generale \sigma \`e il prodotto di \mu_1 \dots \mu_t, l'ordine di \sigma \`e il mcm delle lunghezze dell'ordine dei suoi cicli.

\sigma^j = \mu_1^j \dots \mu_t^j = 1

< \sigma > = \{ \sigma^0, \sigma^1 \dots \sigma^j \}

j deve essere il mcm delle lunghezze di ciascun \mu_i

Esercizio:

H = \{ \sigma \in S_4 t.c. \sigma = 1 oppure \sigma \`e il prodotto di trasposizioni disgiunte \}. H \`e un sottogruppo di S_4?

H = \{ 1, (1 2) (3 4), (1 3) (2 4), (1 4) (2 3) \}

H contiene l'unit\`a, quindi verifica una delle propriet\`a. Verifichiamo se, data una permutazione \sigma, H deve contenere \sigma^{-1}. \`E verificato perch\'e l'inverso di un elemento \`e l'elemento stesso.

In generale l'inverso di un prodotto in un gruppo non commutativo \`e il prodotto al contrario degli inversi. Ma in questo caso sono commutativi i singoli elementi, quindi:

\sigma^{-1} = ((1 2) (3 4) )^{-1} = (3 4)^{-1} (1 2)^{-1} = (4 3) (2 1) = (1 2) (3 4)

Si chiama idempotenza.

\sigma_1 \sigma_2 = \sigma_3 = \sigma_2 \sigma_1

\sigma_1 \sigma_3 = \sigma_1 = \sigma_3 \sigma_1

\sigma_2 \sigma_3 = \sigma_2 = \sigma_3 \sigma_2

Proviamolo con \sigma_1 \sigma_2:

\sigma_1 \sigma_2
1 2 3 4
3 4 1 2
4 3 2 1

(1 4) (2 3)

H \`e un sottogruppo commutativo di un gruppo non commutativo.

L'ordine di H \`e 4. L'ordine di S_4 \`e 4! S_4 pu\`o avere sottogruppi di ordine che divide l'ordine di S_4.

I sottogruppi di H (diversi da H e dall'unit\`a) devono avere ordine 2. I sottogruppi di ordine 2 sono 3: < \sigma_1 > = \{ 1, \sigma_1\}, < \sigma_2 > = \{1, \sigma_2\}, < \sigma_3 > = \{1, \sigma_3 \}.

Se esprimo una \sigma come prodotto di cicli, il prodotto dei cicli \`e commutativo perch\'e i cicli sono disgiunti. Le permutazioni cicliche con elementi in comune non sono commutative.

Esercizio:
Determinare un elemento x di S_8 tale che a x a = a c b a b

Dove a = (1 2 3) (2 3 4) (4 5 6)
non \`e un prodotto di cicli.

b = 
1 2 3 4 5 6 7 8
2 1 5 4 3 7 6 8
dal punto di vista dell'occupazione

c = (2 8)
\`e una trasposizione.

Determinare l'ordine di x, la sua parit\`a e una decomposizione in cicli disgiunti.

Per determinare x dobbiamo fare:

a^{-1} a x a a^{-1} = a^{-1} a c b a b a^{-1}
x = c b a b a^{-1}

% SONO ARRIVATO QUI A SCRIVERE

\section{Strutture algebriche con due operazioni}

\begin{description}
    \item[Anelli] un anello \`e una struttura algebrica $(A, +, \cdot)$ t.c. 
    \begin{enumerate}
        \item La prima operazione $\left( A, + \right )$ \`e un gruppo abeliano.
        \item La seconda operazione considerata sull'insieme escluso l'elemento neutro, $(A \setminus \left \{ 0 \right \}, \cdot )$ \`e un semigruppo.
        \item $ \forall a, b, c \in A , \ a \cdot (b + c) = a \cdot b + a \cdot c $
        \item $ \forall a, b, c \in A , \ (a + b) \cdot c = a \cdot c + b \cdot c $
    \end{enumerate}
    \item[Campi] \`e un anello in cui $( A \setminus \left \{ 0 \right \}, \cdot )$ \`e un gruppo abeliano.
\end{description}

Gli interi sono un anello: $\left ( \mathbb{Z}, +, \cdot \right )$.

I razionali sono un campo: $\left ( \mathbb{Q}, +, \cdot \right )$. Anche $\mathbb{R}$ e $\mathbb{C}$ sono un campo.

% SONO ARRIVATO QUI A SISTEMARE

\subsection{Anelli}

un anello \`e una struttura algebrica con 2 operazioni (A, +, \cdot) tale che:

\begin{description}
    \item[1A] (A, +) \`e un gruppo abeliano (ossia un gruppo commutativo)
    \item[2A] (A, \cdot) \`e un semigruppo, ossia una struttura algebrica associativa
    \item[3A] valgono le propriet\`a distributive (devo scriverle entrambe perch\'e non \`e detto che le operazioni siano associative)

    \forall a b c \in A a (b + c) = ab + ac, (b + c) a = b a + c a
\end{description}

Se l'anello \`e un monoide rispetto al prodotto (ossia ha l'unit\`a), si chiama anello unitario.

Anello unitario \Rightarrow (A, \cdot) \`e un monoide.

Anello commutativo \Rightarrow (A, \cdot) \`e una struttura commutativa.

Anello privo di divisori dello 0, con 0 a indicare l'unit\`a di (A, +). Se a \cdot b = 0 allora a = 0 oppure b = 0 (oppure non esclusivo).

(\mathbb{Z}, +, \cdot) \`e un anello commutativo unitario privo di divisori dello 0.

Prendiamo tutte le funzioni R^R rispetto a + e a \cdot (R^R, +, \cdot)

(f + g) : R \to R \`e definita come (f + g) x = f(x) + g(x) 

(f \cdot g) : R \to R \`e definita in modo naturale anche questa come (f \cdot g) (x) = f(x) \cdot g(x)

Questo \`e un anello. Lo 0 di questo anello rispetto a (R^R, +) \`e \underline{0} : R \to R con \underline{0} + f = f = f + \underline{0}.

\underline{0} (x) = 0

1 : R \to R

1(x) = 1

Ha divisori dello 0. Ne facciamo un esempio, trovarne altri.

f : R \to R
f(x) =
\begin{cases}
x se x = 2n \\
0 altrimenti
\end{cases}

f \neq \underline{0}

Prendiamo una g : R \to R definita cos\`i:

g(x) =
\begin{cases}
x se x = 2n + 1 \\
0 altrimenti
\end{cases}
Anche g \neq \underline{0}

ma (f \cdot g) (x) = 0 \forall x \in X

Quindi f \cdot g = \underline{0}. Abbiamo trovato due elementi del gruppo (R^R, \cdot) il cui prodotto da \underline{0} anche se sono entrambi diversi da \underline{0}.

f e g sono due divisori dello 0.

Teorema di divisione

Dato a \in Z e n > 0 con n \in N, allora esistono due numero q, r \in Z tali che
\begin{description}
    \item a = n \cdot q + r
    \item 0 \le r < n
\end{description}
La coppia q, r \`e unica. Ossia se (q', r') soddisfa 1 e 2, allora q = q' e r = r'.

Facciamo questa dimostrazione usando il principio del buon ordinamento di N. Tutti i sottoinsiemi di N diversi dal vuoto hanno un primo elemento.

n \ge 2

Indichiamo con M = \{ m \in N tali che m = a - n \cdot q con q \in Z \}

Il resto r \`e uno degli elementi di M, in particolare il pi\`u piccolo. M \neq \emptyset, perch\'e se a > 0 \Rightarrow a \in M. Se invece a \le 0 \Rightarrow possiamo fare un piccolo trucchetto. a - n \cdot q = - a \cdot (-1 + n \cdot q) e pongo q = a,

a - n \cdot a = - a \cdot (-1 + n \cdot a)

a - n \cdot a \`e positivo, quindi - a \cdot (-1 + n a) \in M

Siccome M \`e diverso dal vuoto e \`e sottoinsieme di N
M \subseteq N
segue per il principio del buon ordinamento che M ha un primo elemento r (il pi\`u piccolo).

Quindi r = a - n q \Rightarrow a = n q + r.

Dobbiamo dimostrare la seconda propriet\`a, ossia che 0 \le r < n

Supponiamo per assurdo che n \le r \Rightarrow r = n + x con x \le r. Siccome a = n q + r, a = n q + (n + x) = n (q + 1) + x \Rightarrow x \in M e minore di r, quindi ho l'assurdo: r \`e il pi\`u piccolo elemento di M.

Se voglio fare il teorema di divisione con un numero qualunque?

Possiamo esprimere il teorema generale di divisione:

a, b \in Z con b \neq 0 allora \exist q, r \in Z tali che a = q b + r e 0 \le r < |b|. La coppia q, r \`e unica anche in questo caso. Segue come conseguenza dal teorema precedente. 

mcm(a,b) = m 
\Leftrightarrow
\begin{cases}
m \ge 0 
m = k a
m = h b
\`e il sup di a e b nel reticolo della divisibilit\`a: z = k' a e z = h' b \Rightarrow m \le z
\end{cases}

MCD(a,b) = d
\Leftrightarrow
\begin{cases}
d \ge 0
d | a (divide)
d | b
d' | a e d' | b \Rightarrow d' \le d
Quindi d \`e l'inf di a e b nel reticolo di cui sopra.
\end{cases}

Esistenza del minimo comune multiplo. Esiste per il principio del buon ordinamento.

M = \{ t \in N tali che t = k a e t = h b \}, M \`e non vuoto e ha un primo elementi, e quindi esiste il mcm.

Perch\'e \`e non vuoto? Perch\'e a \cdot b \in M. mcm(a,b) \`e il pi\`u piccolo elemento di M.

La dimostrazione per il MCD \`e pi\`u lunga.





















